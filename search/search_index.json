{
    "docs": [
        {
            "location": "/",
            "text": "Guide to using Django with Zappa\n\u00b6\n\n\nThis repo exists to document the process of getting a standard Django project running live in AWS Lambda using the \n\nzappa project\n.  We will explore various configurations in a building-block fashion in the hopes that folks can leverage only the relevant parts for their needs.\n\n\nSetup your Environment\n\u00b6\n\n\nIt is important to read this section in order to establish your working environment: \nSetup your Environment\n\n\nWalkthroughs\n\u00b6\n\n\nCore Django Setup\n\u00b6\n\n\nThis section documents setting up a Django project with only core Python functionality responding to HTTP calls.  The value of this core walkthrough could be to power an API driven compute engine or a event-driven data processing tool without the need to provide a UI.\n\n\nHosting Static Files\n\u00b6\n\n\nGenerally if you'd like to use your Django project to present a User Interface (UI) then you'll need to display Images and CSS and serve Javascript files.  These are known as static files and to deliver them using Zappa is unlike the traditional method of hosting the static files on a Linux or Windows box.  This walkthrough documents one way of hosting the files on AWS S3.\n\n\nUsing a Database\n\u00b6\n\n\nThis walkthough documents the steps necessary to connect your application to a hosted database.\n\n\nUsing a Custom Domain Name\n\u00b6\n\n\nLet's face it, the default urls provided by Zappa via API Gateway are ugly.  Read this walkthrough to get a sense of what it takes to make the urls much more user friendly without a dedicated proxy service.\n\n\nApplication Adaptations\n\u00b6\n\n\nRunning code in AWS Lambda is not the same as running code on a dedicated virtual server.\n\nThis document describes differences in the AWS Lambda environment and outlines many of the possible\nadaptations you may need to apply to your application.",
            "title": "Home"
        },
        {
            "location": "/#guide-to-using-django-with-zappa",
            "text": "This repo exists to document the process of getting a standard Django project running live in AWS Lambda using the  zappa project .  We will explore various configurations in a building-block fashion in the hopes that folks can leverage only the relevant parts for their needs.",
            "title": "Guide to using Django with Zappa"
        },
        {
            "location": "/#setup-your-environment",
            "text": "It is important to read this section in order to establish your working environment:  Setup your Environment",
            "title": "Setup your Environment"
        },
        {
            "location": "/#walkthroughs",
            "text": "",
            "title": "Walkthroughs"
        },
        {
            "location": "/#core-django-setup",
            "text": "This section documents setting up a Django project with only core Python functionality responding to HTTP calls.  The value of this core walkthrough could be to power an API driven compute engine or a event-driven data processing tool without the need to provide a UI.",
            "title": "Core Django Setup"
        },
        {
            "location": "/#hosting-static-files",
            "text": "Generally if you'd like to use your Django project to present a User Interface (UI) then you'll need to display Images and CSS and serve Javascript files.  These are known as static files and to deliver them using Zappa is unlike the traditional method of hosting the static files on a Linux or Windows box.  This walkthrough documents one way of hosting the files on AWS S3.",
            "title": "Hosting Static Files"
        },
        {
            "location": "/#using-a-database",
            "text": "This walkthough documents the steps necessary to connect your application to a hosted database.",
            "title": "Using a Database"
        },
        {
            "location": "/#using-a-custom-domain-name",
            "text": "Let's face it, the default urls provided by Zappa via API Gateway are ugly.  Read this walkthrough to get a sense of what it takes to make the urls much more user friendly without a dedicated proxy service.",
            "title": "Using a Custom Domain Name"
        },
        {
            "location": "/#application-adaptations",
            "text": "Running code in AWS Lambda is not the same as running code on a dedicated virtual server. \nThis document describes differences in the AWS Lambda environment and outlines many of the possible\nadaptations you may need to apply to your application.",
            "title": "Application Adaptations"
        },
        {
            "location": "/setup/",
            "text": "Setup your Environment\n\u00b6\n\n\nThis section provides guidance to set up a zappa working environment.\n\n\nWhy do I need a working environment?\n\u00b6\n\n\nWhile the ultimate goal is to have your Django application hosted in a cloud-based serverless environment, a working environment is needed to:\n\n\n\n\nCollect the required packages\n\n\nBuild a lambda compatible deployment\n\n\nUpload the deployment \n\n\nCoordinate the various AWS services to enable the cloud-based environment\n\n\n\n\nIn addition, a working environment assists with development and testing.  The caveat is that this working environment will not match exactly the cloud-based deployment. However, the goal is to get a reasonablly close approximation while still balancing ease of use.\n\n\nBaseline packages\n\u00b6\n\n\nTo ensure baseline expectations are set, all environments will assume the following criteria:\n\n\n\n\nPython 3.6 or 2.7 (according to \nAWS lambda support\n) \n\n\nDjango 1.10\n\n\nLatest version of \nzappa\n\n\n\n\nIn addition, zappa \nrequires\n a virtual environment in which to function.  So all approaches below include a virtual environment.  \n\n\nApproach #1 - Local Machine\n\u00b6\n\n\nYou can easily set up your working environment on your local machine. For simple projects, this is very easy to manage and maintain.  All you need is Python 2.7, pip, and virtualenv installed.  This works for Windows, MacOS, and Linux machines.  \n\n\nHere we setup a working environment named 'zappatest'\n\n\nmkdir zappatest\n\ncd\n zappatest\nvirtualenv ve\n\nsource\n ve/bin/activate\npip install django zappa\n\n\nAnd you are done.  \n\n\n\n\nWarning\n\n\nWhile this approach is easy to get up and running, the challenge comes along when you require more advanced python packages.  \n\n\nFor example, once you start connecting to databases, you will need to compile packages such as 'psycopg2' for PostGresSQL.  You should consider the implications of installing needed libraries on your local machine.\n\n\nThis is approach is not recommended for any type of serious zappa effort.\n\n\n\n\nApproach #2 - Docker with zappa (recommended)\n\u00b6\n\n\nSometimes leveraging Docker to create an isolated working environment is a good idea.  It takes more work to setup initially, but once you have the foundations, it is quite easy to create multiple working environments and it is easier to share those same environments with other folks on your team.  \n\n\nThe main goal of using Docker is to create an environment that closely matches the AWS lambda environment.  The closer it matches, then there will be less difficult-to-debug problems. \n\n\nWe will leverage the work others have done to enable such an environment.  First and foremost, the folks from \nlambci\n have created github repo called \ndocker-lambda\n that accurately reflects the lambda environment.  It provides:\n\n\n\n\nMultiple uses\n\n\nA 'build' image for compilation, package creation, and deployment\n\n\nA 'run' image for testing and execution of your code\n\n\n\n\n\n\n\n\nFor the purposes of this walkthrough we will focus only on the 'build' image that provides a very nice interactive working environment for zappa.  Further research into how to use the 'run' image is left as an exercise for the reader.\n\n\n\n\nMultiple Python version support\n\n\nPython 2.7 (\nlambci/lambda:build-python2.7\n)\n\n\nPython 3.6 (\nlambci/lambda:build-python3.6\n)\n\n\n\n\n\n\n\n\nNote that this work was originally inspired from \ndanielwhatmuff/zappa\n but has been enhanced to illustrate support for Python 3.6\n\n\nInital Setup\n\u00b6\n\n\nThese steps need to be performed once for a new system\n\n\n\n\nInstall Docker\n\n\nPull the zappa docker image from Docker github\n\n# For Python 2.7 projects\n\ndocker pull lambci/lambda:build-python2.7\n\n# For Python 3.6 projects\n\ndocker pull lambci/lambda:build-python3.6\n\n\n\n\n\nCreate a shortcut that allows AWS credentials to pass through to the docker container\n\n\n\n\n\n\nIf you use \nenvironment variables for AWS Credentials\n then use:\n\nalias\n \nzappashell2\n=\n'docker run -ti -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID -e AWS_DEFAULT_REGION=$AWS_DEFAULT_REGION -v \"$(pwd):/var/task\"  --rm lambci/lambda:build-python2.7 bash'\n\n\nalias\n zappashell2 >> ~/.bash_profile\n\nalias\n \nzappashell3\n=\n'docker run -ti -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID -e AWS_DEFAULT_REGION=$AWS_DEFAULT_REGION -v \"$(pwd):/var/task\"  --rm lambci/lambda:build-python3.6 bash'\n\n\nalias\n zappashell3 >> ~/.bash_profile\n\n\nBe sure to define the \n$AWS_DEFAULT_REGION\n environment variable\n\n\n\n\n\n\nIf you use a \ncredentials file for AWS Credentials\n then use:\n\nalias\n \nzappashell2\n=\n'docker run -ti -e AWS_PROFILE=$AWS_PROFILE -v \"$(pwd):/var/task\" -v ~/.aws/:/root/.aws  --rm lambci/lambda:build-python2.7 bash'\n\n\nalias\n zappashell2 >> ~/.bash_profile\n\nalias\n \nzappashell3\n=\n'docker run -ti -e AWS_PROFILE=$AWS_PROFILE -v \"$(pwd):/var/task\" -v ~/.aws/:/root/.aws  --rm lambci/lambda:build-python3.6 bash'\n\n\nalias\n zappashell3 >> ~/.bash_profile\n\n\nNote that you must either define the \n$AWS_PROFILE\n environment variable or edit the alias above to be hardcoded to a specific profile.  Example of hardcoding the alias:\n\nalias\n \nzappashell3\n=\n'docker run -ti -e AWS_PROFILE=zappa -v \"$(pwd):/var/task\" -v ~/.aws/:/root/.aws  --rm lambci/lambda:build-python3.6 bash'\n\n\n\n\n\n\n\n\n\n\n\n\nTaking a test drive\n\u00b6\n\n\nSo let's try this out now.  Examples going forward will focus on Python 3.6.  To fire up the docker container use:\n\n\n$ \ncd\n /your_zappa_project\n$ zappashell3\nbash-4.2# \n\n\n\n\nNext, create the \nrequired\n virtual environment, activate it, and install needed dependencies\n\n\nbash-4.2# virtualenv ve\nbash-4.2# \nsource\n ve/bin/activate \n\n(\nve\n)\n bash-4.2# pip install -r requirements.txt\n\n\n\n\nSince the virtual environment is contained in the current directory, and the current directory is mapped to your local machine, any changes you make will be persisted between Docker container instances.  But if you depend on libraries that are installed in the system (essentially anything out of the current directory and virtual environment), they will be lost when the container exits.  The solution for this is to create a custom Dockerfile (see below)\n\n\n\n\nWarning\n\n\nIt is very important that you install and activate the virtualenv only in the docker shell.  This will prevent any incompatibilities with the local system environment and the docker environment.\n\n\n\n\nAt this point, you are ready to start using zappa.  Once you are finished, you can simply exit the container.\n\n\nProject Setup\n\u00b6\n\n\nOnce the steps above are complete, then it is very easy to start a working environment.  But generally additional steps are required for package compilation and customizations.\n\n\nCreate a Dockerfile\n\u00b6\n\n\nCreate a local Dockerfile for your project so you can easily modify needed libraries.  Generally this can go in the root of your zappa project.\n\n\nFROM\n lambci/lambda:build-python3.6\n\n\nLABEL \nmaintainer\n=\n\"<your@email.com>\"\n\n\n\nWORKDIR\n /var/task\n\n\n\n# Fancy prompt to remind you are in zappashell\n\n\nRUN\n \necho\n \n'export PS1=\"\\[\\e[36m\\]zappashell>\\[\\e[m\\] \"'\n >> /root/.bashrc\n\n\n# Additional RUN commands here\n\n\n# RUN yum clean all && \\\n\n\n#    yum -y install <stuff>\n\n\n\nCMD\n [\"bash\"]\n\n\n\n\n\nBuild the docker image\n\u00b6\n\n\n$ \ncd\n /your_zappa_project\n$ docker build -t myzappa .\n\n\n\n\nThis will create a local Docker image on your system. \n\n\nUpdate your zappashell alias\n\u00b6\n\n\nTo make sure it points to your new image.  Essentially replace \nlambci/lambda:build-python3.6\n with \nmyzappa\n.  Example:\n\nalias\n \nzappashell\n=\n'docker run -ti -e AWS_PROFILE=zappa -v \"$(pwd):/var/task\" -v ~/.aws/:/root/.aws  --rm myzappa'\n\n\nalias\n zappashell >> ~/.bash_profile\n\n\n\nCreate the Virtual Environment\n\u00b6\n\n\nCreate the \nrequired\n virtual environment, activate it, and install needed dependencies\n\n\n$ zappashell\nzappashell> python -m venv ve\nzappashell> \nsource\n ve/bin/activate \n\n(\nve\n)\n zappa> pip install -r requirements.txt\n\n\n\n\nSince the virtual environment is contained in the current directory, and the current directory is mapped to your local machine, any changes you make will be persisted between Docker container instances.  But if you depend on libraries that are installed in the system (essentially anything out of the current directory and virtual environment), they will be lost when the container exits.  The solution for this is to add these installations as RUN commands in the Dockerfile.\n\n\nUsing your environment\n\u00b6\n\n\nEach time you are working on your project, merely fire up the container:\n\n\n$ \ncd\n /your_zappa_project\n$ zappashell\nzappashell> \nsource\n ve/bin/activate\n\n(\nve\n)\n zappashell> \n\n\n\n\nAll zappa commands can be used to deploy your project:\n\n\n(\nve\n)\n zappashell> zappa info dev",
            "title": "Setup your Environment"
        },
        {
            "location": "/setup/#setup-your-environment",
            "text": "This section provides guidance to set up a zappa working environment.",
            "title": "Setup your Environment"
        },
        {
            "location": "/setup/#why-do-i-need-a-working-environment",
            "text": "While the ultimate goal is to have your Django application hosted in a cloud-based serverless environment, a working environment is needed to:   Collect the required packages  Build a lambda compatible deployment  Upload the deployment   Coordinate the various AWS services to enable the cloud-based environment   In addition, a working environment assists with development and testing.  The caveat is that this working environment will not match exactly the cloud-based deployment. However, the goal is to get a reasonablly close approximation while still balancing ease of use.",
            "title": "Why do I need a working environment?"
        },
        {
            "location": "/setup/#baseline-packages",
            "text": "To ensure baseline expectations are set, all environments will assume the following criteria:   Python 3.6 or 2.7 (according to  AWS lambda support )   Django 1.10  Latest version of  zappa   In addition, zappa  requires  a virtual environment in which to function.  So all approaches below include a virtual environment.",
            "title": "Baseline packages"
        },
        {
            "location": "/setup/#approach-1-local-machine",
            "text": "You can easily set up your working environment on your local machine. For simple projects, this is very easy to manage and maintain.  All you need is Python 2.7, pip, and virtualenv installed.  This works for Windows, MacOS, and Linux machines.    Here we setup a working environment named 'zappatest'  mkdir zappatest cd  zappatest\nvirtualenv ve source  ve/bin/activate\npip install django zappa \nAnd you are done.     Warning  While this approach is easy to get up and running, the challenge comes along when you require more advanced python packages.    For example, once you start connecting to databases, you will need to compile packages such as 'psycopg2' for PostGresSQL.  You should consider the implications of installing needed libraries on your local machine.  This is approach is not recommended for any type of serious zappa effort.",
            "title": "Approach #1 - Local Machine"
        },
        {
            "location": "/setup/#approach-2-docker-with-zappa-recommended",
            "text": "Sometimes leveraging Docker to create an isolated working environment is a good idea.  It takes more work to setup initially, but once you have the foundations, it is quite easy to create multiple working environments and it is easier to share those same environments with other folks on your team.    The main goal of using Docker is to create an environment that closely matches the AWS lambda environment.  The closer it matches, then there will be less difficult-to-debug problems.   We will leverage the work others have done to enable such an environment.  First and foremost, the folks from  lambci  have created github repo called  docker-lambda  that accurately reflects the lambda environment.  It provides:   Multiple uses  A 'build' image for compilation, package creation, and deployment  A 'run' image for testing and execution of your code     For the purposes of this walkthrough we will focus only on the 'build' image that provides a very nice interactive working environment for zappa.  Further research into how to use the 'run' image is left as an exercise for the reader.   Multiple Python version support  Python 2.7 ( lambci/lambda:build-python2.7 )  Python 3.6 ( lambci/lambda:build-python3.6 )     Note that this work was originally inspired from  danielwhatmuff/zappa  but has been enhanced to illustrate support for Python 3.6",
            "title": "Approach #2 - Docker with zappa (recommended)"
        },
        {
            "location": "/setup/#inital-setup",
            "text": "These steps need to be performed once for a new system   Install Docker  Pull the zappa docker image from Docker github # For Python 2.7 projects \ndocker pull lambci/lambda:build-python2.7 # For Python 3.6 projects \ndocker pull lambci/lambda:build-python3.6   Create a shortcut that allows AWS credentials to pass through to the docker container    If you use  environment variables for AWS Credentials  then use: alias   zappashell2 = 'docker run -ti -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID -e AWS_DEFAULT_REGION=$AWS_DEFAULT_REGION -v \"$(pwd):/var/task\"  --rm lambci/lambda:build-python2.7 bash'  alias  zappashell2 >> ~/.bash_profile alias   zappashell3 = 'docker run -ti -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID -e AWS_DEFAULT_REGION=$AWS_DEFAULT_REGION -v \"$(pwd):/var/task\"  --rm lambci/lambda:build-python3.6 bash'  alias  zappashell3 >> ~/.bash_profile \nBe sure to define the  $AWS_DEFAULT_REGION  environment variable    If you use a  credentials file for AWS Credentials  then use: alias   zappashell2 = 'docker run -ti -e AWS_PROFILE=$AWS_PROFILE -v \"$(pwd):/var/task\" -v ~/.aws/:/root/.aws  --rm lambci/lambda:build-python2.7 bash'  alias  zappashell2 >> ~/.bash_profile alias   zappashell3 = 'docker run -ti -e AWS_PROFILE=$AWS_PROFILE -v \"$(pwd):/var/task\" -v ~/.aws/:/root/.aws  --rm lambci/lambda:build-python3.6 bash'  alias  zappashell3 >> ~/.bash_profile \nNote that you must either define the  $AWS_PROFILE  environment variable or edit the alias above to be hardcoded to a specific profile.  Example of hardcoding the alias: alias   zappashell3 = 'docker run -ti -e AWS_PROFILE=zappa -v \"$(pwd):/var/task\" -v ~/.aws/:/root/.aws  --rm lambci/lambda:build-python3.6 bash'",
            "title": "Inital Setup"
        },
        {
            "location": "/setup/#taking-a-test-drive",
            "text": "So let's try this out now.  Examples going forward will focus on Python 3.6.  To fire up the docker container use:  $  cd  /your_zappa_project\n$ zappashell3\nbash-4.2#   Next, create the  required  virtual environment, activate it, and install needed dependencies  bash-4.2# virtualenv ve\nbash-4.2#  source  ve/bin/activate  ( ve )  bash-4.2# pip install -r requirements.txt  Since the virtual environment is contained in the current directory, and the current directory is mapped to your local machine, any changes you make will be persisted between Docker container instances.  But if you depend on libraries that are installed in the system (essentially anything out of the current directory and virtual environment), they will be lost when the container exits.  The solution for this is to create a custom Dockerfile (see below)   Warning  It is very important that you install and activate the virtualenv only in the docker shell.  This will prevent any incompatibilities with the local system environment and the docker environment.   At this point, you are ready to start using zappa.  Once you are finished, you can simply exit the container.",
            "title": "Taking a test drive"
        },
        {
            "location": "/setup/#project-setup",
            "text": "Once the steps above are complete, then it is very easy to start a working environment.  But generally additional steps are required for package compilation and customizations.",
            "title": "Project Setup"
        },
        {
            "location": "/setup/#create-a-dockerfile",
            "text": "Create a local Dockerfile for your project so you can easily modify needed libraries.  Generally this can go in the root of your zappa project.  FROM  lambci/lambda:build-python3.6 \n\nLABEL  maintainer = \"<your@email.com>\"  WORKDIR  /var/task  # Fancy prompt to remind you are in zappashell  RUN   echo   'export PS1=\"\\[\\e[36m\\]zappashell>\\[\\e[m\\] \"'  >> /root/.bashrc # Additional RUN commands here  # RUN yum clean all && \\  #    yum -y install <stuff>  CMD  [\"bash\"]",
            "title": "Create a Dockerfile"
        },
        {
            "location": "/setup/#build-the-docker-image",
            "text": "$  cd  /your_zappa_project\n$ docker build -t myzappa .  This will create a local Docker image on your system.",
            "title": "Build the docker image"
        },
        {
            "location": "/setup/#update-your-zappashell-alias",
            "text": "To make sure it points to your new image.  Essentially replace  lambci/lambda:build-python3.6  with  myzappa .  Example: alias   zappashell = 'docker run -ti -e AWS_PROFILE=zappa -v \"$(pwd):/var/task\" -v ~/.aws/:/root/.aws  --rm myzappa'  alias  zappashell >> ~/.bash_profile",
            "title": "Update your zappashell alias"
        },
        {
            "location": "/setup/#create-the-virtual-environment",
            "text": "Create the  required  virtual environment, activate it, and install needed dependencies  $ zappashell\nzappashell> python -m venv ve\nzappashell>  source  ve/bin/activate  ( ve )  zappa> pip install -r requirements.txt  Since the virtual environment is contained in the current directory, and the current directory is mapped to your local machine, any changes you make will be persisted between Docker container instances.  But if you depend on libraries that are installed in the system (essentially anything out of the current directory and virtual environment), they will be lost when the container exits.  The solution for this is to add these installations as RUN commands in the Dockerfile.",
            "title": "Create the Virtual Environment"
        },
        {
            "location": "/setup/#using-your-environment",
            "text": "Each time you are working on your project, merely fire up the container:  $  cd  /your_zappa_project\n$ zappashell\nzappashell>  source  ve/bin/activate ( ve )  zappashell>   All zappa commands can be used to deploy your project:  ( ve )  zappashell> zappa info dev",
            "title": "Using your environment"
        },
        {
            "location": "/walk_core/",
            "text": "Core Django Setup\n\u00b6\n\n\nThis section documents setting up a Django project with only core Python functionality responding to HTTP calls.  The value of this core walkthrough could be to power an API driven compute engine or a event-driven data processing tool without the need to provide a UI.\n\n\nExpectations and Goals\n\u00b6\n\n\nAfter going through this section the following will work:\n\n\n\n\nURL Routes in your Django projects\n\n\nViews can produce html / json / data output\n\n\nManagement Commands\n\n\n\n\nWhat will not work (yet - see other walkthroughs for this functionality)\n\n\n\n\nStatic Files will not be served (More on that \nhere\n)\n\n\nThere is no database connection available (not even SQLite)\n\n\nCustom domain names (More on that \nhere\n)\n\n\n\n\nSetup AWS Account Credentials\n\u00b6\n\n\nMake sure you setup access to your AWS account from your local command line.  See: \nSetup Local Account Credentials\n\n\nCreate local environment\n\u00b6\n\n\nSee \nSetup your Environment\n\n\nCreate very basic Django project\n\u00b6\n\n\nFor the purposes of this walkthrough we are taking the most basic Django project.  From within your project working directory type the following.  We are creating a fictional Django project called 'frankie'\n\n\ndjango-admin startproject frankie .\n\n\n\n\nTesting the basic Django project\n\u00b6\n\n\nAt this point if you run \n\npython manage.py runserver\n\n\n\nAnd visit http://127.0.0.1:8000 with your browser you should see the standard Django 'It Worked!' page\n\n\nNow quit the server using Control-C.  You should be back at the console prompt\n\n\nSetup Zappa\n\u00b6\n\n\nzappa init\n\n\nYou will encounter a series of prompts:\n\n\n\n\nName of environment - just accept the default 'dev'\n\n\nS3 bucket for deployments.  If the bucket does not exist, zappa will create it for you.  You can use an existing bucket name if you'd like.  Note that this bucket just holds the zappa package temporarily while it is being transferred to AWS lambda.  The zappa package is then removed after deployment.  For the purposes of the walkthrough we are using \nzappatest-code\n\n\nZappa should automatically find the correct Django settings file so accept the default\n\n\nSay 'no' to deploying globally\n\n\nIf everything looks ok, then accept the info\n\n\n\n\nHere's a transcript of what you should see:\n\n\n(ve) $ zappa init\n\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2557\n\u255a\u2550\u2550\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\n  \u2588\u2588\u2588\u2554\u255d \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\n \u2588\u2588\u2588\u2554\u255d  \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u255d \u2588\u2588\u2554\u2550\u2550\u2550\u255d \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2551     \u2588\u2588\u2551     \u2588\u2588\u2551  \u2588\u2588\u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u255d     \u255a\u2550\u255d     \u255a\u2550\u255d  \u255a\u2550\u255d\n\nWelcome to Zappa!\n\nZappa is a system for running server-less Python web applications on AWS Lambda and AWS API Gateway.\nThis `init` command will help you create and configure your new Zappa deployment.\nLet's get started!\n\nYour Zappa configuration can support multiple production environments, like 'dev', 'staging', and 'production'.\nWhat do you want to call this environment (default 'dev'):\n\nYour Zappa deployments will need to be uploaded to a private S3 bucket.\nIf you don't have a bucket yet, we'll create one for you too.\nWhat do you want call your bucket? (default 'zappa-v20ssav8g'): zappatest-code\n\nIt looks like this is a Django application!\nWhat is the module path to your project's Django settings?\nWe discovered: frankie.settings\nWhere are your project's settings? (default 'frankie.settings'):\n\nYou can optionally deploy to all available regions in order to provide fast global service.\nIf you are using Zappa for the first time, you probably don't want to do this!\nWould you like to deploy this application to globally? (default 'n') [y/n/(p)rimary]: n\n\nOkay, here's your zappa_settings.js:\n\n{\n    \"dev\": {\n        \"django_settings\": \"frankie.settings\",\n        \"s3_bucket\": \"zappatest-code\"\n    }\n}\n\nDoes this look okay? (default 'y') [y/n]: y\n\nDone! Now you can deploy your Zappa application by executing:\n\n    $ zappa deploy dev\n\nAfter that, you can update your application code with:\n\n    $ zappa update dev\n\nTo learn more, check out our project page on GitHub here: https://github.com/Miserlou/Zappa\nand stop by our Slack channel here: http://bit.do/zappa\n\nEnjoy!,\n ~ Team Zappa!\n(ve) $\n\n\n\n\nTesting the Zappa Setup\n\u00b6\n\n\nSo now if we run\n\n\nzappa deploy dev\n\n\n\n\nBut unfortunately we encounter an error: \n\n\n(ve) $ zappa deploy dev\nCalling deploy for environment dev..\nWarning! AWS Lambda may not be available in this AWS Region!\nWarning! AWS API Gateway may not be available in this AWS Region!\nOh no! An error occurred! :(\n\n==============\n\nTraceback (most recent call last):\n    [boring callback removed]\nNoRegionError: You must specify a region.\n\n==============\n\nNeed help? Found a bug? Let us know! :D\nFile bug reports on GitHub here: https://github.com/Miserlou/Zappa\nAnd join our Slack channel here: https://slack.zappa.io\nLove!,\n ~ Team Zappa!\n(ve) $\n\n\nAw man, the error \nNoRegionError: You must specify a region.\n is holding us back.  Zappa is complaining that no AWS region is specified.  So we need to specify a region.  In this walkthrough we are leveraging \nus-east-1\n which corresponds to the same region we used above for the S3 bucket.\n\n\nYou have options:\n\n\n\n\n\n\nSpecify a default region using environment variables\n\n\nAgain, the drawback here is this must be set for every console\n\n\nexport AWS_DEFAULT_REGION=us-east-1\n\n\n\n\n\n\n\n\nAdd default region in your \n~/.aws/credentials\n file\n\n\nBetter but this will affect all AWS scripts and programs on your machine.\n\n\n[default]\n\n\naws_access_key_id\n \n=\n \nyour_access_key_id\n\n\naws_secret_access_key\n \n=\n \nyour_secret_access_key\n\n\nregion\n=\nus-east-1\n\n\n\n\n\n\n\n\n\nEdit the \nzappa_settings.json\n file to have an AWS region.\n\n\nProbably best option because now the zappa configuration has minimal dependencies on external user environment.\n\n\n{\n \"dev\": {\n\n     \"aws_region\": \"us-east-1\",\n\n     \"django_settings\": \"frankie.settings\",\n     \"s3_bucket\": \"zappatest-code\"\n        } \n}\n\n\n\n\nDon't forget to put commas in the proper place - JSON is fiddly!\n\n\n\n\n\n\nDeploy your project using Zappa\n\u00b6\n\n\nNow it's easy to do the initial deployment\n\n\nzappa deploy dev\n\n\n\n\nZappa will automatically create an AWS API gateway that will route HTTP requests to your lambda Django project.  You should see something like:\n\n\n(ve) $ zappa deploy dev\nCalling deploy for environment dev..\nDownloading and installing dependencies..\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 27/27 [00:07<00:00,  3.91pkg/s]\nPackaging project as zip..\nUploading zappatest-dev-1482425936.zip (13.1MiB)..\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13.8M/13.8M [00:25<00:00, 603KB/s]\nScheduling..\nScheduled zappatest-dev-zappa-keep-warm-handler.keep_warm_callback!\nUploading zappatest-dev-template-1482425980.json (1.5KiB)..\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.58K/1.58K [00:00<00:00, 2.08KB/s]\nWaiting for stack zappatest-dev to create (this can take a bit)..\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:18<00:00,  4.69s/res]\nDeploying API Gateway..\nDeployment complete!: https://x6kb437rh.execute-api.us-east-1.amazonaws.com/dev\n\n\n\n\nBrilliant!  We should be able to use a browser to visit the URL provided at the end of the script.\n\n\nOnce we do, however, we get:\n\n\nDisallowedHost at /\nInvalid HTTP_HOST header: 'x6kb437rh.execute-api.us-east-1.amazonaws.com'. \nYou may need to add x6kb437rh.execute-api.us-east-1.amazonaws.com' to ALLOWED_HOSTS.\n\n\n\n\nThe built-in \nDjango security settings\n are kicking in and preventing bad stuff from happening.  So we need to modify our Django settings file to accommodate the \ndefault hostname that AWS API Gateway uses\n.  Note that the AWS region is part of the hostname and thus should match your selected region.\n\n\nNow edit \nfrankie/settings.py\n and change ALLOWED_HOSTS to;\n\n\nALLOWED_HOSTS = [ '127.0.0.1', 'x6kb437rh.execute-api.us-east-1.amazonaws.com', ]\n\n\nAs an aside, for best security practices, put the full domain of the API Gateway here.  Less secure would be to use just \n.execute-api.us-east-1.amazonaws.com\n.  \n\n\nOnce done, we can again deploy to AWS Lambda.  But this time, since we've already pushed the initial deploy, we use the \nupdate\n action on the zappa command line.\n\n\nzappa update dev\n\n\n\n\nAfter this completes, you should be able to see your Django site in action.  Note that you will actually get a Page not found (404) response.  This indicates that your Django site is functional and working.   \n\n\n\n\nHow is this functional?\n\u00b6\n\n\nWait, what?  A 404 page is functional?  Well yes, it is.  The Lambda function is working fine.  A whole series of AWS systems are working in concert to load your python Django code and running the view.  Because we've cut to the bare minimum Django project, there is no application ready to handle the url paths.  The only thing we see is the admin application.\n\n\nSo from here we are ready to start working on views and providing data.  However, if you wish to host a website with static files and databases, continue onward to the subsequent walkthroughs:\n\n\n\n\nHosting Static Files\n\n\nUsing a Database\n\n\n\n\nWhy is the URL path appended with 'dev'?\n\u00b6\n\n\nAstute readers will notice that the url in the image shown above indeed has the root domain with the suffix of 'dev' which happens to be the name of the zappa environment.  Indeed, the url domain is based on the generated API Gateway and the path of the URL is the 'Stage Name' of the API Gateway - it matches the name of the Zappa environment you chose above.\n\n\nhttps://bnu0zcwezd.execute-api.us-east-1.amazonaws.com/dev/\n        ^^^^^^^^^^^^^^^^^^^^^^                         ^^^\n      Auto Generated API Gateway              Your Zappa Environment\n\n\n\n\nWhile this url may be considered functional, most would regard it as extremely unfriendly to users. To improve this and use your own custom domain name see the section on \nusing a Custom Domain\n.\n\n\nChecking up on the deployment\n\u00b6\n\n\nIf, at any time, you would like to get information on the deployment, the command to run is\n\n\nzappa status dev\n\n\n\n\nAnd you will get a plethora of data about your deployment:\n\n    Lambda Versions:      2\n    Lambda Name:          zappatest2-dev\n    Lambda ARN:           arn:aws:lambda:us-east-1:738351236015:function:zappatest2-dev\n    Lambda Role ARN:      arn:aws:iam::738351236015:role/ZappaLambdaExecution\n    Lambda Handler:       handler.lambda_handler\n    Lambda Code Size:     11919234\n    Lambda Version:       $LATEST\n    Lambda Last Modified: 2017-04-02T12:56:32.663+0000\n    Lambda Memory Size:   512\n    Lambda Timeout:       30\n    Lambda Runtime:       python2.7\n    Lambda VPC ID:        None\n    Invocations (24h):    6\n    Errors (24h):         0\n    Error Rate (24h):     0.00%\n    API Gateway URL:      https://i1mf39942k.execute-api.us-east-1.amazonaws.com/dev\n    Domain URL:           None Supplied\n    Num. Event Rules:     1\n    Event Rule ARN:       arn:aws:events:us-east-1:1111111111:rule/zappatest2-dev-zappa-keep-warm-handler.keep_warm_callback\n    Event Rule Name:      zappatest2-dev-zappa-keep-warm-handler.keep_warm_callback\n    Event Rule State:     Enabled\n    Event Rule Schedule:  rate(4 minutes)\n\n\nIt includes the API Gateway URL which is important in case you ever forget the URL",
            "title": "Core Django Setup"
        },
        {
            "location": "/walk_core/#core-django-setup",
            "text": "This section documents setting up a Django project with only core Python functionality responding to HTTP calls.  The value of this core walkthrough could be to power an API driven compute engine or a event-driven data processing tool without the need to provide a UI.",
            "title": "Core Django Setup"
        },
        {
            "location": "/walk_core/#expectations-and-goals",
            "text": "After going through this section the following will work:   URL Routes in your Django projects  Views can produce html / json / data output  Management Commands   What will not work (yet - see other walkthroughs for this functionality)   Static Files will not be served (More on that  here )  There is no database connection available (not even SQLite)  Custom domain names (More on that  here )",
            "title": "Expectations and Goals"
        },
        {
            "location": "/walk_core/#setup-aws-account-credentials",
            "text": "Make sure you setup access to your AWS account from your local command line.  See:  Setup Local Account Credentials",
            "title": "Setup AWS Account Credentials"
        },
        {
            "location": "/walk_core/#create-local-environment",
            "text": "See  Setup your Environment",
            "title": "Create local environment"
        },
        {
            "location": "/walk_core/#create-very-basic-django-project",
            "text": "For the purposes of this walkthrough we are taking the most basic Django project.  From within your project working directory type the following.  We are creating a fictional Django project called 'frankie'  django-admin startproject frankie .",
            "title": "Create very basic Django project"
        },
        {
            "location": "/walk_core/#testing-the-basic-django-project",
            "text": "At this point if you run  python manage.py runserver  And visit http://127.0.0.1:8000 with your browser you should see the standard Django 'It Worked!' page  Now quit the server using Control-C.  You should be back at the console prompt",
            "title": "Testing the basic Django project"
        },
        {
            "location": "/walk_core/#setup-zappa",
            "text": "zappa init \nYou will encounter a series of prompts:   Name of environment - just accept the default 'dev'  S3 bucket for deployments.  If the bucket does not exist, zappa will create it for you.  You can use an existing bucket name if you'd like.  Note that this bucket just holds the zappa package temporarily while it is being transferred to AWS lambda.  The zappa package is then removed after deployment.  For the purposes of the walkthrough we are using  zappatest-code  Zappa should automatically find the correct Django settings file so accept the default  Say 'no' to deploying globally  If everything looks ok, then accept the info   Here's a transcript of what you should see:  (ve) $ zappa init\n\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2557\n\u255a\u2550\u2550\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\n  \u2588\u2588\u2588\u2554\u255d \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\n \u2588\u2588\u2588\u2554\u255d  \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u255d \u2588\u2588\u2554\u2550\u2550\u2550\u255d \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2551     \u2588\u2588\u2551     \u2588\u2588\u2551  \u2588\u2588\u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u255d     \u255a\u2550\u255d     \u255a\u2550\u255d  \u255a\u2550\u255d\n\nWelcome to Zappa!\n\nZappa is a system for running server-less Python web applications on AWS Lambda and AWS API Gateway.\nThis `init` command will help you create and configure your new Zappa deployment.\nLet's get started!\n\nYour Zappa configuration can support multiple production environments, like 'dev', 'staging', and 'production'.\nWhat do you want to call this environment (default 'dev'):\n\nYour Zappa deployments will need to be uploaded to a private S3 bucket.\nIf you don't have a bucket yet, we'll create one for you too.\nWhat do you want call your bucket? (default 'zappa-v20ssav8g'): zappatest-code\n\nIt looks like this is a Django application!\nWhat is the module path to your project's Django settings?\nWe discovered: frankie.settings\nWhere are your project's settings? (default 'frankie.settings'):\n\nYou can optionally deploy to all available regions in order to provide fast global service.\nIf you are using Zappa for the first time, you probably don't want to do this!\nWould you like to deploy this application to globally? (default 'n') [y/n/(p)rimary]: n\n\nOkay, here's your zappa_settings.js:\n\n{\n    \"dev\": {\n        \"django_settings\": \"frankie.settings\",\n        \"s3_bucket\": \"zappatest-code\"\n    }\n}\n\nDoes this look okay? (default 'y') [y/n]: y\n\nDone! Now you can deploy your Zappa application by executing:\n\n    $ zappa deploy dev\n\nAfter that, you can update your application code with:\n\n    $ zappa update dev\n\nTo learn more, check out our project page on GitHub here: https://github.com/Miserlou/Zappa\nand stop by our Slack channel here: http://bit.do/zappa\n\nEnjoy!,\n ~ Team Zappa!\n(ve) $",
            "title": "Setup Zappa"
        },
        {
            "location": "/walk_core/#testing-the-zappa-setup",
            "text": "So now if we run  zappa deploy dev  But unfortunately we encounter an error:   (ve) $ zappa deploy dev\nCalling deploy for environment dev..\nWarning! AWS Lambda may not be available in this AWS Region!\nWarning! AWS API Gateway may not be available in this AWS Region!\nOh no! An error occurred! :(\n\n==============\n\nTraceback (most recent call last):\n    [boring callback removed]\nNoRegionError: You must specify a region.\n\n==============\n\nNeed help? Found a bug? Let us know! :D\nFile bug reports on GitHub here: https://github.com/Miserlou/Zappa\nAnd join our Slack channel here: https://slack.zappa.io\nLove!,\n ~ Team Zappa!\n(ve) $ \nAw man, the error  NoRegionError: You must specify a region.  is holding us back.  Zappa is complaining that no AWS region is specified.  So we need to specify a region.  In this walkthrough we are leveraging  us-east-1  which corresponds to the same region we used above for the S3 bucket.  You have options:    Specify a default region using environment variables  Again, the drawback here is this must be set for every console  export AWS_DEFAULT_REGION=us-east-1    Add default region in your  ~/.aws/credentials  file  Better but this will affect all AWS scripts and programs on your machine.  [default]  aws_access_key_id   =   your_access_key_id  aws_secret_access_key   =   your_secret_access_key  region = us-east-1     Edit the  zappa_settings.json  file to have an AWS region.  Probably best option because now the zappa configuration has minimal dependencies on external user environment.  {\n \"dev\": {      \"aws_region\": \"us-east-1\",      \"django_settings\": \"frankie.settings\",\n     \"s3_bucket\": \"zappatest-code\"\n        } \n}  Don't forget to put commas in the proper place - JSON is fiddly!",
            "title": "Testing the Zappa Setup"
        },
        {
            "location": "/walk_core/#deploy-your-project-using-zappa",
            "text": "Now it's easy to do the initial deployment  zappa deploy dev  Zappa will automatically create an AWS API gateway that will route HTTP requests to your lambda Django project.  You should see something like:  (ve) $ zappa deploy dev\nCalling deploy for environment dev..\nDownloading and installing dependencies..\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 27/27 [00:07<00:00,  3.91pkg/s]\nPackaging project as zip..\nUploading zappatest-dev-1482425936.zip (13.1MiB)..\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13.8M/13.8M [00:25<00:00, 603KB/s]\nScheduling..\nScheduled zappatest-dev-zappa-keep-warm-handler.keep_warm_callback!\nUploading zappatest-dev-template-1482425980.json (1.5KiB)..\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.58K/1.58K [00:00<00:00, 2.08KB/s]\nWaiting for stack zappatest-dev to create (this can take a bit)..\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:18<00:00,  4.69s/res]\nDeploying API Gateway..\nDeployment complete!: https://x6kb437rh.execute-api.us-east-1.amazonaws.com/dev  Brilliant!  We should be able to use a browser to visit the URL provided at the end of the script.  Once we do, however, we get:  DisallowedHost at /\nInvalid HTTP_HOST header: 'x6kb437rh.execute-api.us-east-1.amazonaws.com'. \nYou may need to add x6kb437rh.execute-api.us-east-1.amazonaws.com' to ALLOWED_HOSTS.  The built-in  Django security settings  are kicking in and preventing bad stuff from happening.  So we need to modify our Django settings file to accommodate the  default hostname that AWS API Gateway uses .  Note that the AWS region is part of the hostname and thus should match your selected region.  Now edit  frankie/settings.py  and change ALLOWED_HOSTS to;  ALLOWED_HOSTS = [ '127.0.0.1', 'x6kb437rh.execute-api.us-east-1.amazonaws.com', ] \nAs an aside, for best security practices, put the full domain of the API Gateway here.  Less secure would be to use just  .execute-api.us-east-1.amazonaws.com .    Once done, we can again deploy to AWS Lambda.  But this time, since we've already pushed the initial deploy, we use the  update  action on the zappa command line.  zappa update dev  After this completes, you should be able to see your Django site in action.  Note that you will actually get a Page not found (404) response.  This indicates that your Django site is functional and working.",
            "title": "Deploy your project using Zappa"
        },
        {
            "location": "/walk_core/#how-is-this-functional",
            "text": "Wait, what?  A 404 page is functional?  Well yes, it is.  The Lambda function is working fine.  A whole series of AWS systems are working in concert to load your python Django code and running the view.  Because we've cut to the bare minimum Django project, there is no application ready to handle the url paths.  The only thing we see is the admin application.  So from here we are ready to start working on views and providing data.  However, if you wish to host a website with static files and databases, continue onward to the subsequent walkthroughs:   Hosting Static Files  Using a Database",
            "title": "How is this functional?"
        },
        {
            "location": "/walk_core/#why-is-the-url-path-appended-with-dev",
            "text": "Astute readers will notice that the url in the image shown above indeed has the root domain with the suffix of 'dev' which happens to be the name of the zappa environment.  Indeed, the url domain is based on the generated API Gateway and the path of the URL is the 'Stage Name' of the API Gateway - it matches the name of the Zappa environment you chose above.  https://bnu0zcwezd.execute-api.us-east-1.amazonaws.com/dev/\n        ^^^^^^^^^^^^^^^^^^^^^^                         ^^^\n      Auto Generated API Gateway              Your Zappa Environment  While this url may be considered functional, most would regard it as extremely unfriendly to users. To improve this and use your own custom domain name see the section on  using a Custom Domain .",
            "title": "Why is the URL path appended with 'dev'?"
        },
        {
            "location": "/walk_core/#checking-up-on-the-deployment",
            "text": "If, at any time, you would like to get information on the deployment, the command to run is  zappa status dev  And you will get a plethora of data about your deployment:     Lambda Versions:      2\n    Lambda Name:          zappatest2-dev\n    Lambda ARN:           arn:aws:lambda:us-east-1:738351236015:function:zappatest2-dev\n    Lambda Role ARN:      arn:aws:iam::738351236015:role/ZappaLambdaExecution\n    Lambda Handler:       handler.lambda_handler\n    Lambda Code Size:     11919234\n    Lambda Version:       $LATEST\n    Lambda Last Modified: 2017-04-02T12:56:32.663+0000\n    Lambda Memory Size:   512\n    Lambda Timeout:       30\n    Lambda Runtime:       python2.7\n    Lambda VPC ID:        None\n    Invocations (24h):    6\n    Errors (24h):         0\n    Error Rate (24h):     0.00%\n    API Gateway URL:      https://i1mf39942k.execute-api.us-east-1.amazonaws.com/dev\n    Domain URL:           None Supplied\n    Num. Event Rules:     1\n    Event Rule ARN:       arn:aws:events:us-east-1:1111111111:rule/zappatest2-dev-zappa-keep-warm-handler.keep_warm_callback\n    Event Rule Name:      zappatest2-dev-zappa-keep-warm-handler.keep_warm_callback\n    Event Rule State:     Enabled\n    Event Rule Schedule:  rate(4 minutes) \nIt includes the API Gateway URL which is important in case you ever forget the URL",
            "title": "Checking up on the deployment"
        },
        {
            "location": "/walk_static/",
            "text": "Hosting Static Files\n\u00b6\n\n\nGenerally if you'd like to use your Django project to present a User Interface (UI) then you'll need to display Images and CSS and serve Javascript files.  These are known as \nstatic files\n and to deliver them using Zappa is unlike the traditional method of hosting the static files on a Linux or Windows box.  \n\n\nStatic files and Code on a Single Server\n\u00b6\n\n\nA very common configuration you may see recommended is to have your Django project \ndeployed on a server with your static files\n.  Then the advice is to have your web server software (apache, nginx, or other) have special mechanisms to directly serve the static files.  The idea is to have the fast web server software handle delivering the static images to clients and the comparatively slow Django/python code process the more complex views and page content.  \n\n\nBecause Zappa runs in the serverless lambda environment, this approach is not feasible since you cannot configure the web server to handle various url paths differently.  Thus another approach must be taken.\n\n\nLeveraging WSGI app to serve files\n\u00b6\n\n\nThe situation where one does not have access to the web server software configuration is more common than one may think.  Hosting in a shared environment, or on Platform as a Service (PaaS) like OpenShift may prevent full configuration of the web server to effectively serve static files.  \n\n\nThere are ways to leverage the WSGI application (Django for us) and instruct it to serve static files.  Normally, Django treats URL requests as an opportunity to run python code.  And the python code may have complex logic.  But there is a model called \nWhiteNoise\n.  It is an app that will minimize the python code processing to more efficiently serve static files.  Thus no external web server software configuration is required.  While perhaps not as optimal as having the web server hosting the files, this method has been used in production effectively.  \n\n\nUsing external services to serve files\n\u00b6\n\n\nFinally, there is an option to use an \nexternal service to serve static files\n.  This is the option that is the subject of this walkthrough.\n\n\nWhile any external service that serves files over HTTP could work, the focus for us will be to leverage the AWS service of S3 and the Content Delivery Network (CDN) of CloudFront to meet our needs.  \n\n\nThe S3 service will contain our files and provide the fundamental HTTP/HTTPS service.  This alone will suffice for many recreational projects, but more professional project will want to leverage CloudFront to provide caching, faster delivery, and better protection of assets.\n\n\nNote that much of this information was pulled from \nhttps://www.caktusgroup.com/blog/2014/11/10/Using-Amazon-S3-to-store-your-Django-sites-static-and-media-files/\n\n\nUsing a CDN for the entire project\n\u00b6\n\n\nThere are also advantages to serving the entire Django project (Lambda functions and S3 Static files) via the CloudFront CDN.  This option will not be covered in this Walkthrough.\n\n\nSetup and Prerequisites\n\u00b6\n\n\nMake sure you understand and execute the \nCore Django Walkthrough\n first.  This Walkthrough builds upon that.\n\n\nSetup Amazon Account\n\u00b6\n\n\nYou will need an AWS S3 bucket to host your static files.  This should not be the same as your S3 bucket used by zappa to upload your code.  The reason is that you will be making some modifications to the S3 bucket to properly use HTTP to serve files.\n\n\nCreate an S3 bucket and name it something like \nzappa-static\n.  You may name it anything you like but for the purposes of this walkthrough we will use \nzappa-static\n.  Replace all occurrences of this string with your chosen bucket name.\n\n\nConfigure CORS\n\u00b6\n\n\nCORS\n is an HTTP standard that enables browsers to pull content from different sources on a single web page.  Because our Django Lambda views are hosted on a different URL, we must enable the CORS setting on the S3 bucket holding our static assets to allow the files to be pulled.  \n\n\nGo to your S3 bucket properties, and under \"Permissions\", click on \"Add CORS Configuration\". Paste this in:\n\n\n \n<CORSConfiguration>\n\n        \n<CORSRule>\n\n            \n<AllowedOrigin>\n*\n</AllowedOrigin>\n\n            \n<AllowedMethod>\nGET\n</AllowedMethod>\n\n            \n<MaxAgeSeconds>\n3000\n</MaxAgeSeconds>\n\n            \n<AllowedHeader>\nAuthorization\n</AllowedHeader>\n\n        \n</CORSRule>\n\n    \n</CORSConfiguration>\n\n\n\n\nNote that this CORS policy is very open and simple.  If you have a production site, you will probably want to narrow the scope of CORS or leverage a CDN.\n\n\nConfigure Django Project\n\u00b6\n\n\nInstall modules\n\u00b6\n\n\nIn order to re-use existing modules freely available, we will use the \ndjango-s3-storage\n package to handle the management of files to and from AWS S3.  So first you must install it.  \nDon't forget to activate your virtual environment\n \n\n\npip install django-s3-storage\n\n\n\n\nAnd thus you should take the corresponding package versions reported by \npip freeze\n into the requirements.txt file.  At the time of this writing, the additional lines would be:\n\n\n...\ndjango-s3-storage==0.12.4\n...\n\n\n\n\nAdd Django-S3-Storage to the INSTALLED_APPS in settings.py\n\u00b6\n\n\nEdit your settings.py file to include django-s3-storage.  Note it's  called 'django_s3_storage' as an app.\n\n\nINSTALLED_APPS = (\n          ...,\n          'django_s3_storage',\n     )\n\n\n\n\nConfigure Django-S3-Storage in settings.py\n\u00b6\n\n\nAdd these lines anywhere in your settings.py.  These values instruct django-s3-storage to properly configure a basic setup for leveraging S3.  \n\n\nYOUR_S3_BUCKET = \"zappa-static\"\n\nSTATICFILES_STORAGE = \"django_s3_storage.storage.StaticS3Storage\"\nAWS_S3_BUCKET_NAME_STATIC = YOUR_S3_BUCKET\n\n# These next two lines will serve the static files directly \n# from the s3 bucket\nAWS_S3_CUSTOM_DOMAIN = '%s.s3.amazonaws.com' % YOUR_S3_BUCKET\nSTATIC_URL = \"https://%s/\" % AWS_S3_CUSTOM_DOMAIN\n\n# OR...if you create a fancy custom domain for your static files use:\n#AWS_S3_PUBLIC_URL_STATIC = \"https://static.zappaguide.com/\"\n\n\n\n\nPush your static files to the cloud\n\u00b6\n\n\nThe funny thing about zappa is that generally you have a working local environment and a working lambda environment.  In theory either location can push the static files to the cloud.  \n\n\nUsing your local environment:\n\n\npython manage.py collectstatic --noinput\n\n\n\n\nOr to instruct your zappa-powered AWS lambda environment to do it for you (don't forget to push your code changes first)\n\n\nzappa update dev\nzappa manage dev \"collectstatic --noinput\"\n\n\n\n\nTest with the admin\n\u00b6\n\n\nOnce you have pushed your static files to S3, you can visit the admin site for your Django project to test if it worked.  Appending /admin/ to your zappa project you can now browse to the admin site and watch the css being loaded just fine.\n\n\n\n\nNext Steps\n\u00b6\n\n\nWell great, now you have a working Django site that processes views and can serve static files.  But you can't login because there is no database.  Continue through the walkthroughs to complete a fully functional website.\n\n\nAdditional HTTP Settings\n\u00b6\n\n\nAs mentioned above you probably want to ensure a valid CORS policy is in place for anything resembling production.\n\n\nIn addition there are many default HTTP headers that can be served with your static files to ensure proper caching and so forth.  The example format in your settings.py file is:\n\n\nAWS_S3_MAX_AGE_SECONDS_STATIC = \"94608000\"\n\n\n\n\nSee the \ndjango-s3-storage\n for more settings.",
            "title": "Hosting Static Files"
        },
        {
            "location": "/walk_static/#hosting-static-files",
            "text": "Generally if you'd like to use your Django project to present a User Interface (UI) then you'll need to display Images and CSS and serve Javascript files.  These are known as  static files  and to deliver them using Zappa is unlike the traditional method of hosting the static files on a Linux or Windows box.",
            "title": "Hosting Static Files"
        },
        {
            "location": "/walk_static/#static-files-and-code-on-a-single-server",
            "text": "A very common configuration you may see recommended is to have your Django project  deployed on a server with your static files .  Then the advice is to have your web server software (apache, nginx, or other) have special mechanisms to directly serve the static files.  The idea is to have the fast web server software handle delivering the static images to clients and the comparatively slow Django/python code process the more complex views and page content.    Because Zappa runs in the serverless lambda environment, this approach is not feasible since you cannot configure the web server to handle various url paths differently.  Thus another approach must be taken.",
            "title": "Static files and Code on a Single Server"
        },
        {
            "location": "/walk_static/#leveraging-wsgi-app-to-serve-files",
            "text": "The situation where one does not have access to the web server software configuration is more common than one may think.  Hosting in a shared environment, or on Platform as a Service (PaaS) like OpenShift may prevent full configuration of the web server to effectively serve static files.    There are ways to leverage the WSGI application (Django for us) and instruct it to serve static files.  Normally, Django treats URL requests as an opportunity to run python code.  And the python code may have complex logic.  But there is a model called  WhiteNoise .  It is an app that will minimize the python code processing to more efficiently serve static files.  Thus no external web server software configuration is required.  While perhaps not as optimal as having the web server hosting the files, this method has been used in production effectively.",
            "title": "Leveraging WSGI app to serve files"
        },
        {
            "location": "/walk_static/#using-external-services-to-serve-files",
            "text": "Finally, there is an option to use an  external service to serve static files .  This is the option that is the subject of this walkthrough.  While any external service that serves files over HTTP could work, the focus for us will be to leverage the AWS service of S3 and the Content Delivery Network (CDN) of CloudFront to meet our needs.    The S3 service will contain our files and provide the fundamental HTTP/HTTPS service.  This alone will suffice for many recreational projects, but more professional project will want to leverage CloudFront to provide caching, faster delivery, and better protection of assets.  Note that much of this information was pulled from  https://www.caktusgroup.com/blog/2014/11/10/Using-Amazon-S3-to-store-your-Django-sites-static-and-media-files/",
            "title": "Using external services to serve files"
        },
        {
            "location": "/walk_static/#using-a-cdn-for-the-entire-project",
            "text": "There are also advantages to serving the entire Django project (Lambda functions and S3 Static files) via the CloudFront CDN.  This option will not be covered in this Walkthrough.",
            "title": "Using a CDN for the entire project"
        },
        {
            "location": "/walk_static/#setup-and-prerequisites",
            "text": "Make sure you understand and execute the  Core Django Walkthrough  first.  This Walkthrough builds upon that.",
            "title": "Setup and Prerequisites"
        },
        {
            "location": "/walk_static/#setup-amazon-account",
            "text": "You will need an AWS S3 bucket to host your static files.  This should not be the same as your S3 bucket used by zappa to upload your code.  The reason is that you will be making some modifications to the S3 bucket to properly use HTTP to serve files.  Create an S3 bucket and name it something like  zappa-static .  You may name it anything you like but for the purposes of this walkthrough we will use  zappa-static .  Replace all occurrences of this string with your chosen bucket name.",
            "title": "Setup Amazon Account"
        },
        {
            "location": "/walk_static/#configure-cors",
            "text": "CORS  is an HTTP standard that enables browsers to pull content from different sources on a single web page.  Because our Django Lambda views are hosted on a different URL, we must enable the CORS setting on the S3 bucket holding our static assets to allow the files to be pulled.    Go to your S3 bucket properties, and under \"Permissions\", click on \"Add CORS Configuration\". Paste this in:    <CORSConfiguration> \n         <CORSRule> \n             <AllowedOrigin> * </AllowedOrigin> \n             <AllowedMethod> GET </AllowedMethod> \n             <MaxAgeSeconds> 3000 </MaxAgeSeconds> \n             <AllowedHeader> Authorization </AllowedHeader> \n         </CORSRule> \n     </CORSConfiguration>   Note that this CORS policy is very open and simple.  If you have a production site, you will probably want to narrow the scope of CORS or leverage a CDN.",
            "title": "Configure CORS"
        },
        {
            "location": "/walk_static/#configure-django-project",
            "text": "",
            "title": "Configure Django Project"
        },
        {
            "location": "/walk_static/#install-modules",
            "text": "In order to re-use existing modules freely available, we will use the  django-s3-storage  package to handle the management of files to and from AWS S3.  So first you must install it.   Don't forget to activate your virtual environment    pip install django-s3-storage  And thus you should take the corresponding package versions reported by  pip freeze  into the requirements.txt file.  At the time of this writing, the additional lines would be:  ...\ndjango-s3-storage==0.12.4\n...",
            "title": "Install modules"
        },
        {
            "location": "/walk_static/#add-django-s3-storage-to-the-installed_apps-in-settingspy",
            "text": "Edit your settings.py file to include django-s3-storage.  Note it's  called 'django_s3_storage' as an app.  INSTALLED_APPS = (\n          ...,\n          'django_s3_storage',\n     )",
            "title": "Add Django-S3-Storage to the INSTALLED_APPS in settings.py"
        },
        {
            "location": "/walk_static/#configure-django-s3-storage-in-settingspy",
            "text": "Add these lines anywhere in your settings.py.  These values instruct django-s3-storage to properly configure a basic setup for leveraging S3.    YOUR_S3_BUCKET = \"zappa-static\"\n\nSTATICFILES_STORAGE = \"django_s3_storage.storage.StaticS3Storage\"\nAWS_S3_BUCKET_NAME_STATIC = YOUR_S3_BUCKET\n\n# These next two lines will serve the static files directly \n# from the s3 bucket\nAWS_S3_CUSTOM_DOMAIN = '%s.s3.amazonaws.com' % YOUR_S3_BUCKET\nSTATIC_URL = \"https://%s/\" % AWS_S3_CUSTOM_DOMAIN\n\n# OR...if you create a fancy custom domain for your static files use:\n#AWS_S3_PUBLIC_URL_STATIC = \"https://static.zappaguide.com/\"",
            "title": "Configure Django-S3-Storage in settings.py"
        },
        {
            "location": "/walk_static/#push-your-static-files-to-the-cloud",
            "text": "The funny thing about zappa is that generally you have a working local environment and a working lambda environment.  In theory either location can push the static files to the cloud.    Using your local environment:  python manage.py collectstatic --noinput  Or to instruct your zappa-powered AWS lambda environment to do it for you (don't forget to push your code changes first)  zappa update dev\nzappa manage dev \"collectstatic --noinput\"",
            "title": "Push your static files to the cloud"
        },
        {
            "location": "/walk_static/#test-with-the-admin",
            "text": "Once you have pushed your static files to S3, you can visit the admin site for your Django project to test if it worked.  Appending /admin/ to your zappa project you can now browse to the admin site and watch the css being loaded just fine.",
            "title": "Test with the admin"
        },
        {
            "location": "/walk_static/#next-steps",
            "text": "Well great, now you have a working Django site that processes views and can serve static files.  But you can't login because there is no database.  Continue through the walkthroughs to complete a fully functional website.",
            "title": "Next Steps"
        },
        {
            "location": "/walk_static/#additional-http-settings",
            "text": "As mentioned above you probably want to ensure a valid CORS policy is in place for anything resembling production.  In addition there are many default HTTP headers that can be served with your static files to ensure proper caching and so forth.  The example format in your settings.py file is:  AWS_S3_MAX_AGE_SECONDS_STATIC = \"94608000\"  See the  django-s3-storage  for more settings.",
            "title": "Additional HTTP Settings"
        },
        {
            "location": "/walk_database/",
            "text": "Using a Database\n\u00b6\n\n\nThis walkthough documents the steps necessary to connect your application to a hosted database.\n\n\nPrerequisites\n\u00b6\n\n\nThis walkthough requires the \nCore Django Setup\n to be completed.  Also, it is important \nto have your network setup properly so check out \nAdventures in Networking\n.  \n\n\nWe will assume you have chosen the VPC pattern: \"VPC with a Public subnet and Private subnet\"\nBut basically you will need the private subnet or subnets which can access the database.\n\n\nOptions for Databases\n\u00b6\n\n\nUse AWS RDS\n\u00b6\n\n\nThis is probably the easiest to get up and running.  AWS takes care of the messy details of managing the host and provides database-as-a-service (if that's a real thing).  In addition, AWS RDS supports mySQL and PostGreSQL, both which are highly compatible with Django.  \n\n\nHost Your Own\n\u00b6\n\n\nOf course you can be running any type of database on an EC2 instance of your choosing.  Usually an EC2 instance will be associated with one subnet, but it is possible to have multiple IP addresses in different subnets for redundancy.\n\n\nUse another DB Service\n\u00b6\n\n\nThere as some other database services such as DynamoDB.  Depending on the capabilities of the service, you may or may need the subnet information.  \n\n\nProvision your RDS Database in AWS\n\u00b6\n\n\nWe'll just focus on the RDS case for this walkthough.  In fact we'll go through the walkthough using PostGreSQL.\n\n\nSo zip on over to \nCreating an RDS Database\n and set up one.  You should record some key information we'll need here:\n\n\n\n\nThe subnets (there should be at least two) in which we can access the database\n\n\nThe endpoint (hostname) of the database and the port\n\n\nThe username and password for the root user\n\n\n\n\nConfigure RDS security group\n\u00b6\n\n\nBy default newly created RDS Security Groups \nhave no inbound access\n.  So you need\nto make sure your RDS Security group has open TCP connections from your subnets associated with the lambdas.  \n\n\nSo your inbound rules on the RDS security may look like:\n\n\n\n\n\n\n\n\nType\n\n\nProtocol\n\n\nPort Range\n\n\nSource\n\n\n\n\n\n\n\n\n\n\nAll TCP\n\n\nTCP\n\n\n5432\n\n\nsg9a9a1dfc\n\n\n\n\n\n\n\n\nWe open the whole range associated with the security group because when a lambda container is created, it could take any free address\nin the subnet range.\n\n\nSummary Data\n\u00b6\n\n\nNote that at this point you don't yet have a database installed on your RDS instance.  So let's just pick a name we will use for the walkthough.  \n\n\nHere is our sample data:\n\n\n\n\n\n\n\n\nParameter\n\n\nSample value\n\n\n\n\n\n\n\n\n\n\nsubnets\n\n\nsubnet-f3446aba, subnet-c5b8c79e\n\n\n\n\n\n\nsecurity group\n\n\nsg9a9a1dfc\n\n\n\n\n\n\nendpoint\n\n\nzappa-db.crt239fsjdlk.us-east-1.rds.amazonaws.com\n\n\n\n\n\n\ndb username\n\n\nadministrator\n\n\n\n\n\n\ndb password\n\n\nthis_is_not_a_good_password\n\n\n\n\n\n\ndb name\n\n\nzappadbname\n\n\n\n\n\n\n\n\nSetup your Configuration\n\u00b6\n\n\nEdit requirements\n\u00b6\n\n\nNote on PostGreSQL: because the psycopg2 library often involves compiling the library, I would suggest using the \nDocker version of zappa\n to ensure you have isolation of environments and you don't mess up your local system.\n\n\nAdd this to your requirements.txt \n\npsycopg2\n\n\n\nand then\n\n\npip install -r requirements.txt\n\n\n\n\nDjango Settings\n\u00b6\n\n\nAdd the above settings to your settings.py.  This is pretty standard Django db stuff.\n\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.postgresql_psycopg2',\n        'NAME': 'zappadbname',\n        'USER': 'administrator',\n        'PASSWORD': 'this_is_not_a_good_password',\n        'HOST': 'zappa-db.crt239fsjdlk.us-east-1.rds.amazonaws.com',\n        'PORT': '5432',\n    }\n\n}\n\n\n\n\nZappa Settings\n\u00b6\n\n\nNow we add the VPC configuration to our Zappa settings file so that the lambda functions can connect to the database.\n\n\n{\n    \"dev\": {\n        \"django_settings\": \"frankie.settings\", \n        \"s3_bucket\": \"zappatest-code\",\n        \"aws_region\": \"us-east-1\",\n\n        \"vpc_config\" : {\n\n            \"SubnetIds\": [ \"subnet-f3446aba\",\"subnet-c5b8c79e\" ], // use the private subnet\n\n            \"SecurityGroupIds\": [ \"sg-9a9a1dfc\" ]\n\n        }\n\n    }\n}\n\n\n\n\nCreate your Database\n\u00b6\n\n\nOk, easy so far?  Yes!  All we had to do up to this point was carefully click a mouse in the AWS console and \nedit some text files.  Well fun time is over - now we run into some bootstrapping problems.  Fortunately, we only have to do this once each time we need a new database.\n\n\nTurns out that when AWS creates a PostGreSQL RDS instance for you, it doesn't create a database.  So you have to do it yourself.  There are many options, but two methods could be:\n\n\n\n\nUse a db tool on your local machine via a bastion host \n\n\nUse the AWS command line tool\n\n\nWrite some code to setup the database using zappa\n\n\nUse the utility library \nzappa-django-utils\n\n\n\n\nOption 1 is easy if you have the db tool and the bastion host setup.  Option 4 is the easiest and quickest.  But let's explore how to do options two and three. \n\n\n\n\nTip\n\n\nThe quickest and easiest path is to use \nzappa-django-utils\n - especially if you are using a PostGres database.  \n\n\nMany of the functions here are implemented without having to write any of your own code.\n\n\n\n\nUsing AWS command line tool\n\u00b6\n\n\nYou can only use the AWS command line tool to create the database if you are also creating the entire RDS instance.  Be sure to identify the database name to create with '\ndb-name\n'\n\n\nSimply use the \naws\n command tool using syntax similar to\n\nhttp://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateInstance.html#USER_CreateInstance.CLI\n\n\nSetup the Database using zappa\n\u00b6\n\n\nWell, there is no easy way to get this done but here is a possible option: create a management command that can be run in the zappa environment.  \n\n\nCreate a management command in your Django Project\n\u00b6\n\n\nFollow these steps to create a management command environment (make sure your virtualenv is fired up)\n\n\ncd\n frankie\npython manage.py startapp axe\n\ncd\n axe\nmkdir management\n\ncd\n management\ntouch __init__.py\nmkdir commands\n\ncd\n commands\ntouch __init__.py\n\n\n\n\nThen create a file called \ncreate_db.py\n\n\nfrom\n \npsycopg2\n \nimport\n \nconnect\n\n\nfrom\n \npsycopg2.extensions\n \nimport\n \nISOLATION_LEVEL_AUTOCOMMIT\n\n\nfrom\n \ndjango.core.management.base\n \nimport\n \nBaseCommand\n,\n \nCommandError\n\n\nfrom\n \ndjango.conf\n \nimport\n \nsettings\n\n\n\nclass\n \nCommand\n(\nBaseCommand\n):\n\n    \nhelp\n \n=\n \n'Creates the initial database'\n\n\n    \ndef\n \nhandle\n(\nself\n,\n \n*\nargs\n,\n \n**\noptions\n):\n\n        \nself\n.\nstdout\n.\nwrite\n(\nself\n.\nstyle\n.\nSUCCESS\n(\n'Starting db creation'\n))\n\n\n        \ndbname\n \n=\n \nsettings\n.\nDATABASES\n[\n'default'\n][\n'NAME'\n]\n\n        \nuser\n \n=\n \nsettings\n.\nDATABASES\n[\n'default'\n][\n'USER'\n]\n\n        \npassword\n \n=\n \nsettings\n.\nDATABASES\n[\n'default'\n][\n'PASSWORD'\n]\n\n        \nhost\n \n=\n \nsettings\n.\nDATABASES\n[\n'default'\n][\n'HOST'\n]\n\n\n        \ncon\n \n=\n \nNone\n\n        \ncon\n \n=\n \nconnect\n(\ndbname\n=\n'postgres'\n,\n \nuser\n=\nuser\n,\n \nhost\n=\nhost\n,\n \npassword\n=\npassword\n)\n\n        \ncon\n.\nset_isolation_level\n(\nISOLATION_LEVEL_AUTOCOMMIT\n)\n\n        \ncur\n \n=\n \ncon\n.\ncursor\n()\n\n        \ncur\n.\nexecute\n(\n'CREATE DATABASE '\n \n+\n \ndbname\n)\n\n        \ncur\n.\nclose\n()\n\n        \ncon\n.\nclose\n()\n\n\n        \nself\n.\nstdout\n.\nwrite\n(\nself\n.\nstyle\n.\nSUCCESS\n(\n'All Done'\n))\n\n\n\nThen register app axe to settings.py and update files in aws using this command: zappa update dev.\n\n\nRun the management command\n\u00b6\n\n\nzappa manage dev create_db\n\n\n\n\nIf all goes well, then your database should be created.\n\n\nInit the Database\n\u00b6\n\n\nAt this point you should have an empty database ready for your Django application to fill up with schema. If this were a traditional server, you would merely run the \nmigrate\n command.  But you can't because there is no command line.  Thus we have to modify them to adjust to the new environment.\n\n\nSo create your migrations and push the updated code.  \n\n\npython manage.py makemigrations\nzappa update dev\n\n\nNow you invoke the zappa manage command:\n\n\nzappa manage dev migrate\n\n\n\n\nAnd repeat this process every time you make model changes.\n\n\nCreate your Django superuser\n\u00b6\n\n\nThe Django management commands were meant to be run interactively on a command line on a traditional server.  Because there is no command line with lambda, we must do some trickery to get around the input needed for the Django createsuperuser management command.\n\n\nEssentially we will use the \nraw\n flag on the invoke command to just run raw python.  The following command creates a new superuser named 'admin' with email 'admin@yourdomain.com' and password of 'horse battery stapler'\n\n\nzappa invoke --raw dev \n\"from django.contrib.auth.models import User; User.objects.create_superuser('admin', 'admin@yourdomain.com', 'horse battery stapler')\"\n\n\n\n\n\nAdditional superusers can be added via this method or the Django admin console.\n\n\nTest and profit\n\u00b6\n\n\nAt this point you should be able to log into your Django admin:\n\n\n\n(http://marcelog.github.io/articles/aws_lambda_internet_vpc.html)\n\n\n[https://www.isc.upenn.edu/accessing-mysql-databases-aws-python-lambda-function]\n\n\nFurther Topics\n\u00b6\n\n\nSecurity\n\u00b6\n\n\nNotice that we are using the master user credentials for the RDS system.  It would be more secure if we created a dedicated user that can only access the relevant database.  More information on that can be found here:\n\nhttps://www.digitalocean.com/community/tutorials/how-to-use-postgresql-with-your-django-application-on-ubuntu-14-04\n\n\nYou will have to modify your custom Django management command to accommodate creation of a new user.\n\n\nSQLite issues with Python 3\n\u00b6\n\n\nWhile not a hosted service, SQLite often has a lot of value to the Django developer.  There is currently an issue with the AWS Linux Image that uses Python 3 - it does not include the SQLite python connector and thus Django cannot use the SQLite database backend.  This issue is fixed by \nlambda-packages\n which zappa automatically detects and fixes.  However, the lambda-docker project reflects the AWS lambda environment and does not include SQLite.\n\n\nThe recommended solution until AWS Linux Image is updated is:\n   * Download and uncompress the \n_sqlite.so\n from \nhttps://github.com/Miserlou/lambda-packages/files/1425358/_sqlite3.so.zip\n\n   * Place this file in the root of your zappa project\n   * Add an \n\"exclude\" : [\"_sqlite.so\"]\n to your \nzappa_settings.json\n so that it is not unnecessarily included when you deploy your zappa app \n\n\nWith this you should be able to use SQLite with both your lambda-docker environment and lambda deployments.\n\n\nAdditional References\n\u00b6\n\n\nFor MySQL tips:\n\nhttps://www.digitalocean.com/community/tutorials/how-to-use-postgresql-with-your-django-application-on-ubuntu-14-04",
            "title": "Using a Database"
        },
        {
            "location": "/walk_database/#using-a-database",
            "text": "This walkthough documents the steps necessary to connect your application to a hosted database.",
            "title": "Using a Database"
        },
        {
            "location": "/walk_database/#prerequisites",
            "text": "This walkthough requires the  Core Django Setup  to be completed.  Also, it is important \nto have your network setup properly so check out  Adventures in Networking .    We will assume you have chosen the VPC pattern: \"VPC with a Public subnet and Private subnet\"\nBut basically you will need the private subnet or subnets which can access the database.",
            "title": "Prerequisites"
        },
        {
            "location": "/walk_database/#options-for-databases",
            "text": "",
            "title": "Options for Databases"
        },
        {
            "location": "/walk_database/#use-aws-rds",
            "text": "This is probably the easiest to get up and running.  AWS takes care of the messy details of managing the host and provides database-as-a-service (if that's a real thing).  In addition, AWS RDS supports mySQL and PostGreSQL, both which are highly compatible with Django.",
            "title": "Use AWS RDS"
        },
        {
            "location": "/walk_database/#host-your-own",
            "text": "Of course you can be running any type of database on an EC2 instance of your choosing.  Usually an EC2 instance will be associated with one subnet, but it is possible to have multiple IP addresses in different subnets for redundancy.",
            "title": "Host Your Own"
        },
        {
            "location": "/walk_database/#use-another-db-service",
            "text": "There as some other database services such as DynamoDB.  Depending on the capabilities of the service, you may or may need the subnet information.",
            "title": "Use another DB Service"
        },
        {
            "location": "/walk_database/#provision-your-rds-database-in-aws",
            "text": "We'll just focus on the RDS case for this walkthough.  In fact we'll go through the walkthough using PostGreSQL.  So zip on over to  Creating an RDS Database  and set up one.  You should record some key information we'll need here:   The subnets (there should be at least two) in which we can access the database  The endpoint (hostname) of the database and the port  The username and password for the root user",
            "title": "Provision your RDS Database in AWS"
        },
        {
            "location": "/walk_database/#configure-rds-security-group",
            "text": "By default newly created RDS Security Groups  have no inbound access .  So you need\nto make sure your RDS Security group has open TCP connections from your subnets associated with the lambdas.    So your inbound rules on the RDS security may look like:     Type  Protocol  Port Range  Source      All TCP  TCP  5432  sg9a9a1dfc     We open the whole range associated with the security group because when a lambda container is created, it could take any free address\nin the subnet range.",
            "title": "Configure RDS security group"
        },
        {
            "location": "/walk_database/#summary-data",
            "text": "Note that at this point you don't yet have a database installed on your RDS instance.  So let's just pick a name we will use for the walkthough.    Here is our sample data:     Parameter  Sample value      subnets  subnet-f3446aba, subnet-c5b8c79e    security group  sg9a9a1dfc    endpoint  zappa-db.crt239fsjdlk.us-east-1.rds.amazonaws.com    db username  administrator    db password  this_is_not_a_good_password    db name  zappadbname",
            "title": "Summary Data"
        },
        {
            "location": "/walk_database/#setup-your-configuration",
            "text": "",
            "title": "Setup your Configuration"
        },
        {
            "location": "/walk_database/#edit-requirements",
            "text": "Note on PostGreSQL: because the psycopg2 library often involves compiling the library, I would suggest using the  Docker version of zappa  to ensure you have isolation of environments and you don't mess up your local system.  Add this to your requirements.txt  psycopg2  and then  pip install -r requirements.txt",
            "title": "Edit requirements"
        },
        {
            "location": "/walk_database/#django-settings",
            "text": "Add the above settings to your settings.py.  This is pretty standard Django db stuff.  DATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.postgresql_psycopg2',\n        'NAME': 'zappadbname',\n        'USER': 'administrator',\n        'PASSWORD': 'this_is_not_a_good_password',\n        'HOST': 'zappa-db.crt239fsjdlk.us-east-1.rds.amazonaws.com',\n        'PORT': '5432',\n    }\n\n}",
            "title": "Django Settings"
        },
        {
            "location": "/walk_database/#zappa-settings",
            "text": "Now we add the VPC configuration to our Zappa settings file so that the lambda functions can connect to the database.  {\n    \"dev\": {\n        \"django_settings\": \"frankie.settings\", \n        \"s3_bucket\": \"zappatest-code\",\n        \"aws_region\": \"us-east-1\",         \"vpc_config\" : {             \"SubnetIds\": [ \"subnet-f3446aba\",\"subnet-c5b8c79e\" ], // use the private subnet             \"SecurityGroupIds\": [ \"sg-9a9a1dfc\" ]         }     }\n}",
            "title": "Zappa Settings"
        },
        {
            "location": "/walk_database/#create-your-database",
            "text": "Ok, easy so far?  Yes!  All we had to do up to this point was carefully click a mouse in the AWS console and \nedit some text files.  Well fun time is over - now we run into some bootstrapping problems.  Fortunately, we only have to do this once each time we need a new database.  Turns out that when AWS creates a PostGreSQL RDS instance for you, it doesn't create a database.  So you have to do it yourself.  There are many options, but two methods could be:   Use a db tool on your local machine via a bastion host   Use the AWS command line tool  Write some code to setup the database using zappa  Use the utility library  zappa-django-utils   Option 1 is easy if you have the db tool and the bastion host setup.  Option 4 is the easiest and quickest.  But let's explore how to do options two and three.    Tip  The quickest and easiest path is to use  zappa-django-utils  - especially if you are using a PostGres database.    Many of the functions here are implemented without having to write any of your own code.",
            "title": "Create your Database"
        },
        {
            "location": "/walk_database/#using-aws-command-line-tool",
            "text": "You can only use the AWS command line tool to create the database if you are also creating the entire RDS instance.  Be sure to identify the database name to create with ' db-name '  Simply use the  aws  command tool using syntax similar to http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateInstance.html#USER_CreateInstance.CLI",
            "title": "Using AWS command line tool"
        },
        {
            "location": "/walk_database/#setup-the-database-using-zappa",
            "text": "Well, there is no easy way to get this done but here is a possible option: create a management command that can be run in the zappa environment.",
            "title": "Setup the Database using zappa"
        },
        {
            "location": "/walk_database/#create-a-management-command-in-your-django-project",
            "text": "Follow these steps to create a management command environment (make sure your virtualenv is fired up)  cd  frankie\npython manage.py startapp axe cd  axe\nmkdir management cd  management\ntouch __init__.py\nmkdir commands cd  commands\ntouch __init__.py  Then create a file called  create_db.py  from   psycopg2   import   connect  from   psycopg2.extensions   import   ISOLATION_LEVEL_AUTOCOMMIT  from   django.core.management.base   import   BaseCommand ,   CommandError  from   django.conf   import   settings  class   Command ( BaseCommand ): \n     help   =   'Creates the initial database' \n\n     def   handle ( self ,   * args ,   ** options ): \n         self . stdout . write ( self . style . SUCCESS ( 'Starting db creation' )) \n\n         dbname   =   settings . DATABASES [ 'default' ][ 'NAME' ] \n         user   =   settings . DATABASES [ 'default' ][ 'USER' ] \n         password   =   settings . DATABASES [ 'default' ][ 'PASSWORD' ] \n         host   =   settings . DATABASES [ 'default' ][ 'HOST' ] \n\n         con   =   None \n         con   =   connect ( dbname = 'postgres' ,   user = user ,   host = host ,   password = password ) \n         con . set_isolation_level ( ISOLATION_LEVEL_AUTOCOMMIT ) \n         cur   =   con . cursor () \n         cur . execute ( 'CREATE DATABASE '   +   dbname ) \n         cur . close () \n         con . close () \n\n         self . stdout . write ( self . style . SUCCESS ( 'All Done' ))  \nThen register app axe to settings.py and update files in aws using this command: zappa update dev.",
            "title": "Create a management command in your Django Project"
        },
        {
            "location": "/walk_database/#run-the-management-command",
            "text": "zappa manage dev create_db  If all goes well, then your database should be created.",
            "title": "Run the management command"
        },
        {
            "location": "/walk_database/#init-the-database",
            "text": "At this point you should have an empty database ready for your Django application to fill up with schema. If this were a traditional server, you would merely run the  migrate  command.  But you can't because there is no command line.  Thus we have to modify them to adjust to the new environment.  So create your migrations and push the updated code.    python manage.py makemigrations\nzappa update dev \nNow you invoke the zappa manage command:  zappa manage dev migrate  And repeat this process every time you make model changes.",
            "title": "Init the Database"
        },
        {
            "location": "/walk_database/#create-your-django-superuser",
            "text": "The Django management commands were meant to be run interactively on a command line on a traditional server.  Because there is no command line with lambda, we must do some trickery to get around the input needed for the Django createsuperuser management command.  Essentially we will use the  raw  flag on the invoke command to just run raw python.  The following command creates a new superuser named 'admin' with email 'admin@yourdomain.com' and password of 'horse battery stapler'  zappa invoke --raw dev  \"from django.contrib.auth.models import User; User.objects.create_superuser('admin', 'admin@yourdomain.com', 'horse battery stapler')\"   Additional superusers can be added via this method or the Django admin console.",
            "title": "Create your Django superuser"
        },
        {
            "location": "/walk_database/#test-and-profit",
            "text": "At this point you should be able to log into your Django admin:  (http://marcelog.github.io/articles/aws_lambda_internet_vpc.html)  [https://www.isc.upenn.edu/accessing-mysql-databases-aws-python-lambda-function]",
            "title": "Test and profit"
        },
        {
            "location": "/walk_database/#further-topics",
            "text": "",
            "title": "Further Topics"
        },
        {
            "location": "/walk_database/#security",
            "text": "Notice that we are using the master user credentials for the RDS system.  It would be more secure if we created a dedicated user that can only access the relevant database.  More information on that can be found here: https://www.digitalocean.com/community/tutorials/how-to-use-postgresql-with-your-django-application-on-ubuntu-14-04  You will have to modify your custom Django management command to accommodate creation of a new user.",
            "title": "Security"
        },
        {
            "location": "/walk_database/#sqlite-issues-with-python-3",
            "text": "While not a hosted service, SQLite often has a lot of value to the Django developer.  There is currently an issue with the AWS Linux Image that uses Python 3 - it does not include the SQLite python connector and thus Django cannot use the SQLite database backend.  This issue is fixed by  lambda-packages  which zappa automatically detects and fixes.  However, the lambda-docker project reflects the AWS lambda environment and does not include SQLite.  The recommended solution until AWS Linux Image is updated is:\n   * Download and uncompress the  _sqlite.so  from  https://github.com/Miserlou/lambda-packages/files/1425358/_sqlite3.so.zip \n   * Place this file in the root of your zappa project\n   * Add an  \"exclude\" : [\"_sqlite.so\"]  to your  zappa_settings.json  so that it is not unnecessarily included when you deploy your zappa app   With this you should be able to use SQLite with both your lambda-docker environment and lambda deployments.",
            "title": "SQLite issues with Python 3"
        },
        {
            "location": "/walk_database/#additional-references",
            "text": "For MySQL tips: https://www.digitalocean.com/community/tutorials/how-to-use-postgresql-with-your-django-application-on-ubuntu-14-04",
            "title": "Additional References"
        },
        {
            "location": "/walk_domain/",
            "text": "Using a Custom Domain\n\u00b6\n\n\nIf you've followed the walkthroughs thus far, you've at least created a working \nDjango site using Zappa\n\nBut the URL provided by Zappa is pretty darn ugly.  Not only does it use an apparent random domain name, but the Zappa environment is used as the path.  For example:\n\n\nhttps://bnu0zcwezd.execute-api.us-east-1.amazonaws.com/dev/\n        ^^^^^^^^^^^^^^^^^^^^^^                         ^^^\n      Auto Generated API Gateway              Your Zappa Environment \n\n\n\n\nIdeally most sites would be something like:\n\n\nhttps://www.zappaguide.com/\n\n\n\n\nThis is entirely possible with Zappa - so how do we get there?\n\n\nLet's talk about HTTPS\n\u00b6\n\n\nPerhaps you're wondering why we are introducting the concept of HTTPS when the topic of this walkthough is using a custom domain.  Zappa provides an automated way of creating the necessary custom domain mappings as a part of using encryption.  Thus many of the techiques described in this walkthough will ultimately end up with a custom domain along with HTTPS.  \n\n\nIn an effort to make the process straightforward, we are therefore bundling HTTPS as part of the walkthough.  Philosophical arguments for HTTPS are made \nelsewhere\n.  But with free services like \"Let's Encrypt\" and AWS Certificate Manager (free for API Gateways) there is no additional cost burden to leverage HTTPS certificates.\n\n\nNote that we refer to HTTPS instead of SSL and or TLS where \nappropriate\n.\n\n\nAs a final note, if you are really opposed to encryption, or need unencrypted traffic for some reason, we will provide a method to accomplish this at the end of the walkthrough.\n\n\nOverview of the process\n\u00b6\n\n\nThere are a number of services that are involved in this process:\n\n\n\n\nDomain Name Registrar\n - Allows you to purchase and register domain names\n\n\nDNS Providers\n - Allows you to host things online using that domain name\n\n\nCertificate Authority (CA)\n - Provides encryption certifications to encrypt traffic for the site\n\n\n\n\nCombined with Zappa, these services will all be used in this walkthrough.  Note that many companies and organizations can provide these services and some, like Amazon, can provide all three.\n\n\nUltimately, the AWS API Gateway will be associated with a new, dedicated CloudFront distribution that not only leverages the digital certificate to provide HTTPS, but also hides the Zappa environment path.  Finally, a DNS record will point to this new CloudFront Distro to complete the experience for the end user.\n\n\nRegistering your Custom Domain\n\u00b6\n\n\nFirst you need a registered domain.  It doesn't matter who your domain registrar is as long as you have control over the name server records to point to a DNS provider.  Of all the services, this one is the most generic and almost any Registar will do.\n\n\nLet's choose an example domain for this walkthrough:\n\nwww.zappaguide.com\n\n\n\nChoices\n\u00b6\n\n\nAt this point, you have a registered domain name and a working Zappa deployment.  There are two options:\n\n\n\n\n\n\nUse the built-in Zappa commands\n \n\n\nThe Zappa project has a very easy way of associating your custom domain name with your Zappa deployment.  For most circumstances, this will meet the needs of most applications.  \n\n\nWhat happens behind the scenes is that Zappa tells the AWS API Gateway to associate a private AWS CloudFront distribution with the Custom Domain along with an HTTPS certificate.  This CloudFront distribution cannot be configured, but will faithfully pass along HTTP requests as needed.\n\n\nIt's easy to use and gets you up and running quickly.\n\n\n\n\n\n\nManage your own CloudFront Distribution\n\n\nThe private CloudFront distribution created with the API Gateway is fine, but sometimes you need more control. \nThe alternative is to create your own AWS CloudFront Distribution. \nBy doing this, you still associate a Custom Domain Name with HTTPS, but you unlock the full power of AWS CloudFront. \n\n\n\n\n\n\nUsing the built-in Zappa commands\n\u00b6\n\n\nZappa has some built-in functionality that streamlines the process of associating a Custom Domain Name with your Zappa deployment.  Since there are so many service providers, we focus on a couple combinations that work best.  Use the chart below to select the scenario that best matches your situation and follow only one set of instructions.\n\n\n\n\n\n\n\n\nDNS Provider\n\n\nCA\n\n\nNotes\n\n\nInstructions\n\n\n\n\n\n\n\n\n\n\nRoute53\n\n\nAWS Certificate Manager\n\n\nAll AWS combo makes this ridiculous easy\n\n\nsee below\n\n\n\n\n\n\nRoute53\n\n\nLet's Encrypt\n\n\nAn option that Zappa has smoothed the way, but may be deprecated in the future\n\n\nsee below\n\n\n\n\n\n\nOther DNS\n\n\nACM or Let's Encrypt\n\n\nThere are more manual steps\n\n\nsee below\n\n\n\n\n\n\nOther DNS\n\n\nOther\n\n\nYou got some work to do\n\n\nsee below\n\n\n\n\n\n\n\n\nOption 1: Route53 and ACM\n\u00b6\n\n\nThis option assumes that you will be using AWS Route53 and Amazon Certficate Manager for all functions, except perhaps the domain registration itself.  Therefore any domain registrar will work under this option be it NameCheap, GoDaddy, or anyone else.  Of course the domain name can be registered with Route53.\n\n\nStep 1.1: Create a Hosted Zone in Route53\n\u00b6\n\n\nIf your Registrar is also Route53, skip this step and move on to Step 2.  AWS did this for you when you registered the domain.\n\n\nFollow the instructions for \ncreating a hosted zone in Route53\n\n\nStep 1.2: Create your digital certificate in ACM\n\u00b6\n\n\nFollow the instructions for \nrequesting a certificate in the ACM console\n\n\nBe sure to record the ARN for the newly issued certificate.\n\n\nStep 1.3: Edit the Zappa Settings File\n\u00b6\n\n\nNow we add the following to our Zappa settings file.  These settings prepare Zappa to configure our API gateway properly.\n\n\n{\n    \"dev\": {\n        \"django_settings\": \"frankie.settings\", \n        \"s3_bucket\": \"zappatest-code\",\n        \"aws_region\": \"us-east-1\",\n        \"vpc_config\" : {\n            \"SubnetIds\": [ \"subnet-f3446aba\",\"subnet-c5b8c79e\" ], // use the private subnet\n            \"SecurityGroupIds\": [ \"sg-9a9a1dfc\" ]\n        },\n\n        \"certificate_arn\": \"arn:aws:acm:us-east-1:738356466015:certificate/1d066282-ce94-4ad7-a802-2ff87d32b104\",\n\n        \"domain\": \"www.zappaguide.com\",\n\n    }\n}\n\n\n\n\nFor the \ncertificate_arn\n use the ARN value obtained in step 2 above.  For the \ndomain\n here we could choose either \nwww.zappaguide.com\n or \nzappaguide.com\n, but not both.  In order to handle both, either a redirect must occur or you can setup another \nCloudFront Distribution manually\n.\n\n\nStep 1.4: Run Certify\n\u00b6\n\n\nThis final step triggers your local Zappa environment to reach out to AWS and configure your API Gateway to honor the domain name specified.\n\n\n(ve) $ zappa certify dev\nCalling certify for environment dev..\nAre you sure you want to certify? [y/n] y\nCertifying domain www.zappaguide.com..\nCreated a new domain name with supplied certificate. Please note that it can take up to 40 minutes for this domain to be created and propagated through AWS, but it requires no further work on your part.\nCertificate updated!\n(ve) $ \n\n\n\n\nAnd that should work fine going forward\n\n\n\n\nNote\n\n\nAmazon official documentation states that this step could take up to 40 minutes to initialize the certificate.\n\n\n\n\n\n\nNote\n\n\nThe cloudfront distro associated with the zappa API gateway is hidden from the CF distro tool and is exclusively associated with the API gateway. In addition, you cannot configure any settings on it.\n\n\n\n\n\n\nWarning\n\n\nThis command must be run in the US East (N. Virginia) (us-east-1).  See \nAWS documentation\n for more details.\n\n\n\n\nOption 2: Route53 and Let's Encrypt\n\u00b6\n\n\n\n\nWarning\n\n\nAs of Fall/Winter 2017, there has been discussion by Zappa developers that this option may ultimately be deprecated and removed.\n\n\n\n\nStep 2.1: Create a Hosted Zone in Route53\n\u00b6\n\n\nIf your Registrar is also Route53, skip this step and move on to Step 2.  AWS did this for you when you registered the domain.\n\n\nFollow the instructions for \ncreating a hosted zone in Route53\n\n\nStep 2.2: Create an AWS RSA Key\n\u00b6\n\n\nZappa will interact automatically with Let's Encrypt on your behalf, but first you must create an RSA key to identify your account to Let's Encrypt.\n\n\nTo generate it, simply run:\n\n(ve) $ openssl genrsa -out le-account.key 2048 \nGenerating RSA private key, 2048 bit long modulus\n...........................................................................................................................+++\n..........+++\ne is 65537 (0x10001)\n(ve) $\n\n\n\nBe sure to protect this key because it will enable HTTPS certificates to be generated and you will not be able to update an HTTPS certificate if you lose it.\n\n\n\n\nNote\n\n\nNote that this is a 2048b key. It's generally preferred to use a stronger 4096b key, but AWS does not yet support keys larger than 2048b.\n\n\n\n\nStep 2.3: Edit the Zappa Settings File\n\u00b6\n\n\nNow we add the following to our Zappa settings file.  These settings prepare Zappa to configure our API gateway properly.\n\n\n{\n    \"dev\": {\n        \"django_settings\": \"frankie.settings\", \n        \"s3_bucket\": \"zappatest-code\",\n        \"aws_region\": \"us-east-1\",\n        \"vpc_config\" : {\n            \"SubnetIds\": [ \"subnet-f3446aba\",\"subnet-c5b8c79e\" ], // use the private subnet\n            \"SecurityGroupIds\": [ \"sg-9a9a1dfc\" ]\n        },\n\n        \"lets_encrypt_key\": \"le-account.key\", // Local path to account key - can also be s3 path\n\n        \"domain\": \"www.zappaguide.com\",\n\n    }\n}\n\n\n\n\nStep 2.4: Run Certify\n\u00b6\n\n\nThis final step triggers your local Zappa environment to reach out to AWS and configure your API Gateway to honor the domain name specified.\n\n\n(ve) $ zappa certify dev\nCalling certify for environment dev..\nAre you sure you want to certify? [y/n] y\nCertifying domain www.zappaguide.com..\nCreated a new domain name with supplied certificate. Please note that it can take up to 40 minutes for this domain to be created and propagated through AWS, but it requires no further work on your part.\nCertificate updated!\n(ve) $ \n\n\n\n\nAnd that should work fine going forward.  Note that Let's Encrypt certificates only last for 3 months so you should ensure you update the certificate before the 3 months expire.\n\n\n\n\nNote\n\n\nAmazon official documentation states that this step could take up to 40 minutes to initialize the certificate.\n\n\n\n\nOther Service Providers\n\u00b6\n\n\nIf you choose to use your own DNS provider and/or your own Certificate Authority to create the custom domain names, you will have to perform the manual steps outlined in the official AWS documentation:\n\n\nhttp://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-custom-domains.html#how-to-custom-domains-console\n\n\nIn this case, I would recommend against using the built-in Zappa commands because of unexpected side effects.\n\n\n\n\nWhat's the path of least resistance?\n\n\nGiven these conditions, you should seriously consider a \n\ncustom CloudFront Distribution\n.  Managing\nyour own CloudFront Distribution is not a lot of work, there are many benefits, and the available\ndocumentation is more abundant and complete.\n\n\n\n\nTroubleshooting\n\u00b6\n\n\nUsing your own domain name can be one of the most frustrating experiences, especially due to the potential for a long delay while AWS is creating/setting up the necessary components.  Here we list some of the common errors that you may get when you think everything is working.\n\n\n403 - Forbidden!\n\u00b6\n\n\nSometimes you may encounter the dreaded '403 - Forbidden message': \n\n\n{\"message\":\"Forbidden\"}\n\n\n\n\nOften this happens when the user runs \nzappa certify\n but has not completed all the steps or correctly configured the zappa settings file.  The best way to handle this is generally to manually remove any partially configured custom domains from the AWS console and then try to run \nzappa certify\n again.  Follow these steps to remove any partial custom domain remnants.  \n\n\nStep 1 - Browse to API Gateway -> Custom Domain\n\u00b6\n\n\n\n\nStep 2 - Remove the Custom Domain Mapping\n\u00b6\n\n\n\n\nStep 3 - Re-run \nzappa certify\n\u00b6\n\n\nDjango is redirecting to the raw url\n\u00b6\n\n\nAnother mistake often seen is that when a form is submitted or another HTTP redirect happens, the URL generated is no longer the custom domain, but rather the 'raw' API Gateway URL.  If you see this, most often you are missing the \ndomain\n parameter in the zappa settings file.\n\n\nManage Your Own CloudFront Distribution\n\u00b6\n\n\nAs mentioned, sometimes you may run into the limitations of the CloudFront Distro created with API Gateway.\nOf course this depends on what requirements your project needs in your scenarios.\nThe AWS CloudFront service is marketed as a Content Delivery Network (CDN), traditionally used to serve static content to users. In reality, \nCloudFront\n has rich and \nfull-featured set of capabilities to deliver\nalmost any kind of data including dynamic data from a website. \n\n\nAdditional functionality when you use your own CloudFront distributions include:\n\n\n\n\n\n\nAggregate Static and Dynamic Content\n - If you are serving static content \nfrom S3 or other \nstatic services\n, you can use a custom CloudFront Distro to have a single domain name\nfor your entire website by using \nmultiple origins\n.\nThis sometimes simplifies technologies that are easier to configure with a single domain policy (e.g. CORS, HTTPS, etc).\nAlso busy sites may get a more cost effective delivery mechanism than serving directly from S3.  This is because the CloudFront per byte\ndelivery is lower than directly from S3 and a good caching strategy will allow those assets to be served from CloudFront instead of S3.\n\n\n\n\n\n\nExtensive cache control\n - \nThis will let you configure caching timeouts for multiple paths. So if you have a fairly static landing page, the cache timeout could be days or weeks; while the user account page may have cache of seconds or minutes. Advanced caching could include query parameters and/or cookies.  While some\nframeworks like Django have very good cache header controls, other Python frameworks do not.\n\n\n\n\n\n\nGeo Restrictions\n - CloudFront has the ability to \nrestrict and/or modify content based on geographic location\n.  Using your own CloudFront Distro enables you \nto control this feature.\n\n\n\n\n\n\nEnhanced Security\n - If your application requires increased security, then you can leverage both \n\nAWS Web Application Firewall (WAF)\n\nand \nAWS Shield (Managed DDOS Protection)\n. But you cannot use these services without creating your\nown custom CloudFront. \n\n\n\n\n\n\nServing Private Content\n - CloudFront supports the \nability to create signed URLs and cookies\n\n that allow only authorized end users to access content (e.g. videos, images, etc). \n\n\n\n\n\n\nDeployment Flexiblity\n - If your project tends to have a lot of changes and there is a possiblity that you may wish to switch between\nzappa deployments \nwithout significant downtime\n then you'll want your own CloudFront Distro.\n\nRunning \ncertify\n on a new zappa deployment could take up to 40 minutes,\nand requires the domain and ssl certs, thus causing a service outage if the domain is in use.\nHaving a custom CloudFront Distro allows you to switch the origin path between\nzappa deployments with only having to consider cache timeouts. \n\n\n\n\n\n\nBasically if you'd like to leverage some of the powerful features and tools not exposed by default.  See the \n\nAWS CloudFront documentation\n for more information.\n\n\nCreate a CloudFront Distribution\n\u00b6\n\n\nTo get started, follow \nthese instructions\n\n\nSome key parameters:\n\n\n\n\nSelect 'Web' Distribution\n\n\nFor Origin Domain Name, use the Zappa distribution domain name (e.g. 'bnu0zcwezd.execute-api.us-east-1.amazonaws.com')\n\n\nFor Origin Path, use the Zappa deployment name (e.g. 'dev')\n\n\nFor Object Caching:\n\n\nIf you'd like to use Django to control the cache, select 'Use Origin Cache Headers'\n\n\nIf you'd like to setup static cache timeouts, select 'Customize'\n\n\nUse the Minimum TTL, Maximum TTL, and Default TTL to specify how long (in seconds) to cache objects\n\n\nYou can add additional paths as needed\n\n\n\n\n\n\n\n\n\n\nCompress Objects Automatically, we recommend True\n\n\n\n\nAssociate HTTPS certificate\n\u00b6\n\n\nFollow \nthese instructions\n to associate your new distro with a SSL/TLS certificate.\n\n\nThen create additional 'Origins'\n\u00b6\n\n\nSo now that you have your default origin configured you can add additional ones.  And you can point to various url paths in your application to configure the cache timings and other behavoirs like compression and so on.",
            "title": "Custom Domains"
        },
        {
            "location": "/walk_domain/#using-a-custom-domain",
            "text": "If you've followed the walkthroughs thus far, you've at least created a working  Django site using Zappa \nBut the URL provided by Zappa is pretty darn ugly.  Not only does it use an apparent random domain name, but the Zappa environment is used as the path.  For example:  https://bnu0zcwezd.execute-api.us-east-1.amazonaws.com/dev/\n        ^^^^^^^^^^^^^^^^^^^^^^                         ^^^\n      Auto Generated API Gateway              Your Zappa Environment   Ideally most sites would be something like:  https://www.zappaguide.com/  This is entirely possible with Zappa - so how do we get there?",
            "title": "Using a Custom Domain"
        },
        {
            "location": "/walk_domain/#lets-talk-about-https",
            "text": "Perhaps you're wondering why we are introducting the concept of HTTPS when the topic of this walkthough is using a custom domain.  Zappa provides an automated way of creating the necessary custom domain mappings as a part of using encryption.  Thus many of the techiques described in this walkthough will ultimately end up with a custom domain along with HTTPS.    In an effort to make the process straightforward, we are therefore bundling HTTPS as part of the walkthough.  Philosophical arguments for HTTPS are made  elsewhere .  But with free services like \"Let's Encrypt\" and AWS Certificate Manager (free for API Gateways) there is no additional cost burden to leverage HTTPS certificates.  Note that we refer to HTTPS instead of SSL and or TLS where  appropriate .  As a final note, if you are really opposed to encryption, or need unencrypted traffic for some reason, we will provide a method to accomplish this at the end of the walkthrough.",
            "title": "Let's talk about HTTPS"
        },
        {
            "location": "/walk_domain/#overview-of-the-process",
            "text": "There are a number of services that are involved in this process:   Domain Name Registrar  - Allows you to purchase and register domain names  DNS Providers  - Allows you to host things online using that domain name  Certificate Authority (CA)  - Provides encryption certifications to encrypt traffic for the site   Combined with Zappa, these services will all be used in this walkthrough.  Note that many companies and organizations can provide these services and some, like Amazon, can provide all three.  Ultimately, the AWS API Gateway will be associated with a new, dedicated CloudFront distribution that not only leverages the digital certificate to provide HTTPS, but also hides the Zappa environment path.  Finally, a DNS record will point to this new CloudFront Distro to complete the experience for the end user.",
            "title": "Overview of the process"
        },
        {
            "location": "/walk_domain/#registering-your-custom-domain",
            "text": "First you need a registered domain.  It doesn't matter who your domain registrar is as long as you have control over the name server records to point to a DNS provider.  Of all the services, this one is the most generic and almost any Registar will do.  Let's choose an example domain for this walkthrough: www.zappaguide.com",
            "title": "Registering your Custom Domain"
        },
        {
            "location": "/walk_domain/#choices",
            "text": "At this point, you have a registered domain name and a working Zappa deployment.  There are two options:    Use the built-in Zappa commands    The Zappa project has a very easy way of associating your custom domain name with your Zappa deployment.  For most circumstances, this will meet the needs of most applications.    What happens behind the scenes is that Zappa tells the AWS API Gateway to associate a private AWS CloudFront distribution with the Custom Domain along with an HTTPS certificate.  This CloudFront distribution cannot be configured, but will faithfully pass along HTTP requests as needed.  It's easy to use and gets you up and running quickly.    Manage your own CloudFront Distribution  The private CloudFront distribution created with the API Gateway is fine, but sometimes you need more control. \nThe alternative is to create your own AWS CloudFront Distribution. \nBy doing this, you still associate a Custom Domain Name with HTTPS, but you unlock the full power of AWS CloudFront.",
            "title": "Choices"
        },
        {
            "location": "/walk_domain/#using-the-built-in-zappa-commands",
            "text": "Zappa has some built-in functionality that streamlines the process of associating a Custom Domain Name with your Zappa deployment.  Since there are so many service providers, we focus on a couple combinations that work best.  Use the chart below to select the scenario that best matches your situation and follow only one set of instructions.     DNS Provider  CA  Notes  Instructions      Route53  AWS Certificate Manager  All AWS combo makes this ridiculous easy  see below    Route53  Let's Encrypt  An option that Zappa has smoothed the way, but may be deprecated in the future  see below    Other DNS  ACM or Let's Encrypt  There are more manual steps  see below    Other DNS  Other  You got some work to do  see below",
            "title": "Using the built-in Zappa commands"
        },
        {
            "location": "/walk_domain/#option-1-route53-and-acm",
            "text": "This option assumes that you will be using AWS Route53 and Amazon Certficate Manager for all functions, except perhaps the domain registration itself.  Therefore any domain registrar will work under this option be it NameCheap, GoDaddy, or anyone else.  Of course the domain name can be registered with Route53.",
            "title": "Option 1: Route53 and ACM"
        },
        {
            "location": "/walk_domain/#step-11-create-a-hosted-zone-in-route53",
            "text": "If your Registrar is also Route53, skip this step and move on to Step 2.  AWS did this for you when you registered the domain.  Follow the instructions for  creating a hosted zone in Route53",
            "title": "Step 1.1: Create a Hosted Zone in Route53"
        },
        {
            "location": "/walk_domain/#step-12-create-your-digital-certificate-in-acm",
            "text": "Follow the instructions for  requesting a certificate in the ACM console  Be sure to record the ARN for the newly issued certificate.",
            "title": "Step 1.2: Create your digital certificate in ACM"
        },
        {
            "location": "/walk_domain/#step-13-edit-the-zappa-settings-file",
            "text": "Now we add the following to our Zappa settings file.  These settings prepare Zappa to configure our API gateway properly.  {\n    \"dev\": {\n        \"django_settings\": \"frankie.settings\", \n        \"s3_bucket\": \"zappatest-code\",\n        \"aws_region\": \"us-east-1\",\n        \"vpc_config\" : {\n            \"SubnetIds\": [ \"subnet-f3446aba\",\"subnet-c5b8c79e\" ], // use the private subnet\n            \"SecurityGroupIds\": [ \"sg-9a9a1dfc\" ]\n        },         \"certificate_arn\": \"arn:aws:acm:us-east-1:738356466015:certificate/1d066282-ce94-4ad7-a802-2ff87d32b104\",         \"domain\": \"www.zappaguide.com\",     }\n}  For the  certificate_arn  use the ARN value obtained in step 2 above.  For the  domain  here we could choose either  www.zappaguide.com  or  zappaguide.com , but not both.  In order to handle both, either a redirect must occur or you can setup another  CloudFront Distribution manually .",
            "title": "Step 1.3: Edit the Zappa Settings File"
        },
        {
            "location": "/walk_domain/#step-14-run-certify",
            "text": "This final step triggers your local Zappa environment to reach out to AWS and configure your API Gateway to honor the domain name specified.  (ve) $ zappa certify dev\nCalling certify for environment dev..\nAre you sure you want to certify? [y/n] y\nCertifying domain www.zappaguide.com..\nCreated a new domain name with supplied certificate. Please note that it can take up to 40 minutes for this domain to be created and propagated through AWS, but it requires no further work on your part.\nCertificate updated!\n(ve) $   And that should work fine going forward   Note  Amazon official documentation states that this step could take up to 40 minutes to initialize the certificate.    Note  The cloudfront distro associated with the zappa API gateway is hidden from the CF distro tool and is exclusively associated with the API gateway. In addition, you cannot configure any settings on it.    Warning  This command must be run in the US East (N. Virginia) (us-east-1).  See  AWS documentation  for more details.",
            "title": "Step 1.4: Run Certify"
        },
        {
            "location": "/walk_domain/#option-2-route53-and-lets-encrypt",
            "text": "Warning  As of Fall/Winter 2017, there has been discussion by Zappa developers that this option may ultimately be deprecated and removed.",
            "title": "Option 2: Route53 and Let's Encrypt"
        },
        {
            "location": "/walk_domain/#step-21-create-a-hosted-zone-in-route53",
            "text": "If your Registrar is also Route53, skip this step and move on to Step 2.  AWS did this for you when you registered the domain.  Follow the instructions for  creating a hosted zone in Route53",
            "title": "Step 2.1: Create a Hosted Zone in Route53"
        },
        {
            "location": "/walk_domain/#step-22-create-an-aws-rsa-key",
            "text": "Zappa will interact automatically with Let's Encrypt on your behalf, but first you must create an RSA key to identify your account to Let's Encrypt.  To generate it, simply run: (ve) $ openssl genrsa -out le-account.key 2048 \nGenerating RSA private key, 2048 bit long modulus\n...........................................................................................................................+++\n..........+++\ne is 65537 (0x10001)\n(ve) $  Be sure to protect this key because it will enable HTTPS certificates to be generated and you will not be able to update an HTTPS certificate if you lose it.   Note  Note that this is a 2048b key. It's generally preferred to use a stronger 4096b key, but AWS does not yet support keys larger than 2048b.",
            "title": "Step 2.2: Create an AWS RSA Key"
        },
        {
            "location": "/walk_domain/#step-23-edit-the-zappa-settings-file",
            "text": "Now we add the following to our Zappa settings file.  These settings prepare Zappa to configure our API gateway properly.  {\n    \"dev\": {\n        \"django_settings\": \"frankie.settings\", \n        \"s3_bucket\": \"zappatest-code\",\n        \"aws_region\": \"us-east-1\",\n        \"vpc_config\" : {\n            \"SubnetIds\": [ \"subnet-f3446aba\",\"subnet-c5b8c79e\" ], // use the private subnet\n            \"SecurityGroupIds\": [ \"sg-9a9a1dfc\" ]\n        },         \"lets_encrypt_key\": \"le-account.key\", // Local path to account key - can also be s3 path         \"domain\": \"www.zappaguide.com\",     }\n}",
            "title": "Step 2.3: Edit the Zappa Settings File"
        },
        {
            "location": "/walk_domain/#step-24-run-certify",
            "text": "This final step triggers your local Zappa environment to reach out to AWS and configure your API Gateway to honor the domain name specified.  (ve) $ zappa certify dev\nCalling certify for environment dev..\nAre you sure you want to certify? [y/n] y\nCertifying domain www.zappaguide.com..\nCreated a new domain name with supplied certificate. Please note that it can take up to 40 minutes for this domain to be created and propagated through AWS, but it requires no further work on your part.\nCertificate updated!\n(ve) $   And that should work fine going forward.  Note that Let's Encrypt certificates only last for 3 months so you should ensure you update the certificate before the 3 months expire.   Note  Amazon official documentation states that this step could take up to 40 minutes to initialize the certificate.",
            "title": "Step 2.4: Run Certify"
        },
        {
            "location": "/walk_domain/#other-service-providers",
            "text": "If you choose to use your own DNS provider and/or your own Certificate Authority to create the custom domain names, you will have to perform the manual steps outlined in the official AWS documentation:  http://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-custom-domains.html#how-to-custom-domains-console  In this case, I would recommend against using the built-in Zappa commands because of unexpected side effects.   What's the path of least resistance?  Given these conditions, you should seriously consider a  custom CloudFront Distribution .  Managing\nyour own CloudFront Distribution is not a lot of work, there are many benefits, and the available\ndocumentation is more abundant and complete.",
            "title": "Other Service Providers"
        },
        {
            "location": "/walk_domain/#troubleshooting",
            "text": "Using your own domain name can be one of the most frustrating experiences, especially due to the potential for a long delay while AWS is creating/setting up the necessary components.  Here we list some of the common errors that you may get when you think everything is working.",
            "title": "Troubleshooting"
        },
        {
            "location": "/walk_domain/#403-forbidden",
            "text": "Sometimes you may encounter the dreaded '403 - Forbidden message':   {\"message\":\"Forbidden\"}  Often this happens when the user runs  zappa certify  but has not completed all the steps or correctly configured the zappa settings file.  The best way to handle this is generally to manually remove any partially configured custom domains from the AWS console and then try to run  zappa certify  again.  Follow these steps to remove any partial custom domain remnants.",
            "title": "403 - Forbidden!"
        },
        {
            "location": "/walk_domain/#step-1-browse-to-api-gateway-custom-domain",
            "text": "",
            "title": "Step 1 - Browse to API Gateway -&gt; Custom Domain"
        },
        {
            "location": "/walk_domain/#step-2-remove-the-custom-domain-mapping",
            "text": "",
            "title": "Step 2 - Remove the Custom Domain Mapping"
        },
        {
            "location": "/walk_domain/#step-3-re-run-zappa-certify",
            "text": "",
            "title": "Step 3 - Re-run zappa certify"
        },
        {
            "location": "/walk_domain/#django-is-redirecting-to-the-raw-url",
            "text": "Another mistake often seen is that when a form is submitted or another HTTP redirect happens, the URL generated is no longer the custom domain, but rather the 'raw' API Gateway URL.  If you see this, most often you are missing the  domain  parameter in the zappa settings file.",
            "title": "Django is redirecting to the raw url"
        },
        {
            "location": "/walk_domain/#manage-your-own-cloudfront-distribution",
            "text": "As mentioned, sometimes you may run into the limitations of the CloudFront Distro created with API Gateway.\nOf course this depends on what requirements your project needs in your scenarios.\nThe AWS CloudFront service is marketed as a Content Delivery Network (CDN), traditionally used to serve static content to users. In reality,  CloudFront  has rich and \nfull-featured set of capabilities to deliver\nalmost any kind of data including dynamic data from a website.   Additional functionality when you use your own CloudFront distributions include:    Aggregate Static and Dynamic Content  - If you are serving static content  from S3 or other \nstatic services , you can use a custom CloudFront Distro to have a single domain name\nfor your entire website by using  multiple origins .\nThis sometimes simplifies technologies that are easier to configure with a single domain policy (e.g. CORS, HTTPS, etc).\nAlso busy sites may get a more cost effective delivery mechanism than serving directly from S3.  This is because the CloudFront per byte\ndelivery is lower than directly from S3 and a good caching strategy will allow those assets to be served from CloudFront instead of S3.    Extensive cache control  - \nThis will let you configure caching timeouts for multiple paths. So if you have a fairly static landing page, the cache timeout could be days or weeks; while the user account page may have cache of seconds or minutes. Advanced caching could include query parameters and/or cookies.  While some\nframeworks like Django have very good cache header controls, other Python frameworks do not.    Geo Restrictions  - CloudFront has the ability to  restrict and/or modify content based on geographic location .  Using your own CloudFront Distro enables you \nto control this feature.    Enhanced Security  - If your application requires increased security, then you can leverage both  AWS Web Application Firewall (WAF) \nand  AWS Shield (Managed DDOS Protection) . But you cannot use these services without creating your\nown custom CloudFront.     Serving Private Content  - CloudFront supports the  ability to create signed URLs and cookies \n that allow only authorized end users to access content (e.g. videos, images, etc).     Deployment Flexiblity  - If your project tends to have a lot of changes and there is a possiblity that you may wish to switch between\nzappa deployments  without significant downtime  then you'll want your own CloudFront Distro. \nRunning  certify  on a new zappa deployment could take up to 40 minutes,\nand requires the domain and ssl certs, thus causing a service outage if the domain is in use.\nHaving a custom CloudFront Distro allows you to switch the origin path between\nzappa deployments with only having to consider cache timeouts.     Basically if you'd like to leverage some of the powerful features and tools not exposed by default.  See the  AWS CloudFront documentation  for more information.",
            "title": "Manage Your Own CloudFront Distribution"
        },
        {
            "location": "/walk_domain/#create-a-cloudfront-distribution",
            "text": "To get started, follow  these instructions  Some key parameters:   Select 'Web' Distribution  For Origin Domain Name, use the Zappa distribution domain name (e.g. 'bnu0zcwezd.execute-api.us-east-1.amazonaws.com')  For Origin Path, use the Zappa deployment name (e.g. 'dev')  For Object Caching:  If you'd like to use Django to control the cache, select 'Use Origin Cache Headers'  If you'd like to setup static cache timeouts, select 'Customize'  Use the Minimum TTL, Maximum TTL, and Default TTL to specify how long (in seconds) to cache objects  You can add additional paths as needed      Compress Objects Automatically, we recommend True",
            "title": "Create a CloudFront Distribution"
        },
        {
            "location": "/walk_domain/#associate-https-certificate",
            "text": "Follow  these instructions  to associate your new distro with a SSL/TLS certificate.",
            "title": "Associate HTTPS certificate"
        },
        {
            "location": "/walk_domain/#then-create-additional-origins",
            "text": "So now that you have your default origin configured you can add additional ones.  And you can point to various url paths in your application to configure the cache timings and other behavoirs like compression and so on.",
            "title": "Then create additional 'Origins'"
        },
        {
            "location": "/walk_app/",
            "text": "Adapting your Application to Zappa / Lambda\n\u00b6\n\n\nIf you've done the walkthroughs thus far, they have allowed you to seamlessly get your existing application\n(or allowed you to create a new one) in AWS Lambda using \nZappa\n.  \n\n\nBut running code in AWS Lambda is not the same as running code on a dedicated virtual server.\n\nThis document describes differences in the AWS Lambda environment and outlines many of the possible\nadaptations you may need to apply to your Application.\n\n\nMinimal disk storage might be persistent\n\u00b6\n\n\nWhen your code runs, whether it was trigged from an HTTP request or other event, the only disk storage\navailable to write files is in \n/tmp\n.  And that storage is limited to 500MB.  Always check the \nAWS Lambda Limits\n page because this limitation could change over time.\n\n\nInterestingly, once your code runs and is complete, the AWS Lamda service 'freezes' your container and if trigged again, \ncould 'unfreeze' the container for reuse\n.  What this means from a\npractical standpoint is that if you need to write out files to \n/tmp\n your code could both find files\nfrom previous runs and also run out of disk space.\n\n\nThe obivous adaptations when writing out files:\n\n\n\n\nDo not rely on having files exist between code invocations\n\n\nIf the temp files are of significant size, it would be better to clean them up on exit to avoid future code invocations from running out of space\n\n\nAny content uploaded via HTTP (or downloaded/created during invocation) must be persisted elsewhere such as in S3\n\n\nIf you need unique files on disk for each invocation, be sure the space required per invocation\nmultipied by the number of possible invocations is less than 500MB.\n\n\n\n\nSome additional use cases:\n\n\nPoor Man's Search Engline\n\u00b6\n\n\nYou could use the temp space to power file-based tools such as the \nWhoosh Search Engine\n by downloading the search index from S3.  This may work for small indexes.\n\n\nPassing Environment Variables to your Application\n\u00b6\n\n\nThere are a number of ways to pass information to your application\n\n\nEnvironment Variables in Zappa Settings\n\u00b6\n\n\nYou can include variables in the zappa settings file directly.  These variables are easy to set and are included in each deployment.  Great for variables that do not change often or are not sensitive (e.g. credentials).\n\n\n{\n    \"dev\": {\n        ...\n        \"environment_variables\": {\n            \"some_key\": \"some_value\"\n        }\n    },\n    ...\n}\n\n\n\n\nAnd then you can easily retrieve the information from within your code:\n\n\nimport\n \nos\n\n\nsome_value\n \n=\n \nos\n.\nenviron\n.\nget\n(\n'some_key'\n)\n\n\n\n\n\nLambda Environment Variables\n\u00b6\n\n\nYour code can pull information from the execution environment by using the built-in AWS Lambda environment variables.  There is also a method for adding custom variables via the AWS Console.\nThis method is generally only useful for system-generated variables since custom variables can more easily be configured in zappa settings (see above).\n\n\nFirst, there are the \nstandard environment variables\n such as path to code, region, and python path.  These values are automatically calculated and driven by AWS.  \n\n\nIn addition to these system variables, you can set \ncustom environment variables in the AWS Console\n.\n\nSo you could add \nSOME_LAMBDA_KEY\n in the AWS console and retrieve it in your code:\n\n\nimport\n \nos\n\n\nsome_lamda_key\n \n=\n \nos\n.\nenviron\n.\nget\n(\n'SOME_LAMBDA_KEY'\n)\n\n\n# or get system values\n\n\naws_lambda_function_name\n \n=\n \nos\n.\nenviron\n.\nget\n(\n'AWS_LAMBDA_FUNCTION_NAME'\n)\n\n\n\n\n\nWhile on the topic of system-generated information, your code can also pull important information from \nthe \nPython execution context\n:\n\n\n\n\nWhile a Lambda function is executing, it can interact with the AWS Lambda service to get useful runtime information such as:\n\n\n\n\nHow much time is remaining before AWS Lambda terminates your Lambda function (timeout is one of the Lambda function configuration properties).\n\n\nThe CloudWatch log group and log stream associated with the Lambda function that is executing.\n\n\nThe AWS request ID returned to the client that invoked the Lambda function. You can use the request ID for any follow up inquiry with AWS support.\n\n\nIf the Lambda function is invoked through AWS Mobile SDK, you can learn more about the mobile application calling the Lambda function.",
            "title": "Application Adaptations"
        },
        {
            "location": "/walk_app/#adapting-your-application-to-zappa-lambda",
            "text": "If you've done the walkthroughs thus far, they have allowed you to seamlessly get your existing application\n(or allowed you to create a new one) in AWS Lambda using  Zappa .    But running code in AWS Lambda is not the same as running code on a dedicated virtual server. \nThis document describes differences in the AWS Lambda environment and outlines many of the possible\nadaptations you may need to apply to your Application.",
            "title": "Adapting your Application to Zappa / Lambda"
        },
        {
            "location": "/walk_app/#minimal-disk-storage-might-be-persistent",
            "text": "When your code runs, whether it was trigged from an HTTP request or other event, the only disk storage\navailable to write files is in  /tmp .  And that storage is limited to 500MB.  Always check the  AWS Lambda Limits  page because this limitation could change over time.  Interestingly, once your code runs and is complete, the AWS Lamda service 'freezes' your container and if trigged again,  could 'unfreeze' the container for reuse .  What this means from a\npractical standpoint is that if you need to write out files to  /tmp  your code could both find files\nfrom previous runs and also run out of disk space.  The obivous adaptations when writing out files:   Do not rely on having files exist between code invocations  If the temp files are of significant size, it would be better to clean them up on exit to avoid future code invocations from running out of space  Any content uploaded via HTTP (or downloaded/created during invocation) must be persisted elsewhere such as in S3  If you need unique files on disk for each invocation, be sure the space required per invocation\nmultipied by the number of possible invocations is less than 500MB.   Some additional use cases:",
            "title": "Minimal disk storage might be persistent"
        },
        {
            "location": "/walk_app/#poor-mans-search-engline",
            "text": "You could use the temp space to power file-based tools such as the  Whoosh Search Engine  by downloading the search index from S3.  This may work for small indexes.",
            "title": "Poor Man's Search Engline"
        },
        {
            "location": "/walk_app/#passing-environment-variables-to-your-application",
            "text": "There are a number of ways to pass information to your application",
            "title": "Passing Environment Variables to your Application"
        },
        {
            "location": "/walk_app/#environment-variables-in-zappa-settings",
            "text": "You can include variables in the zappa settings file directly.  These variables are easy to set and are included in each deployment.  Great for variables that do not change often or are not sensitive (e.g. credentials).  {\n    \"dev\": {\n        ...\n        \"environment_variables\": {\n            \"some_key\": \"some_value\"\n        }\n    },\n    ...\n}  And then you can easily retrieve the information from within your code:  import   os  some_value   =   os . environ . get ( 'some_key' )",
            "title": "Environment Variables in Zappa Settings"
        },
        {
            "location": "/walk_app/#lambda-environment-variables",
            "text": "Your code can pull information from the execution environment by using the built-in AWS Lambda environment variables.  There is also a method for adding custom variables via the AWS Console.\nThis method is generally only useful for system-generated variables since custom variables can more easily be configured in zappa settings (see above).  First, there are the  standard environment variables  such as path to code, region, and python path.  These values are automatically calculated and driven by AWS.    In addition to these system variables, you can set  custom environment variables in the AWS Console . \nSo you could add  SOME_LAMBDA_KEY  in the AWS console and retrieve it in your code:  import   os  some_lamda_key   =   os . environ . get ( 'SOME_LAMBDA_KEY' )  # or get system values  aws_lambda_function_name   =   os . environ . get ( 'AWS_LAMBDA_FUNCTION_NAME' )   While on the topic of system-generated information, your code can also pull important information from \nthe  Python execution context :   While a Lambda function is executing, it can interact with the AWS Lambda service to get useful runtime information such as:   How much time is remaining before AWS Lambda terminates your Lambda function (timeout is one of the Lambda function configuration properties).  The CloudWatch log group and log stream associated with the Lambda function that is executing.  The AWS request ID returned to the client that invoked the Lambda function. You can use the request ID for any follow up inquiry with AWS support.  If the Lambda function is invoked through AWS Mobile SDK, you can learn more about the mobile application calling the Lambda function.",
            "title": "Lambda Environment Variables"
        },
        {
            "location": "/aws_credentials/",
            "text": "Managing AWS Credentials\n\u00b6\n\n\nGetting Started with AWS and Zappa\n\u00b6\n\n\nDetails in this section are light because this information is documented well elsewhere on the web.\n\n\n\n\nCreate AWS Account if you haven't already\n\n\nCreate an S3 bucket.\n\n   For purposes of this walkthrough I have used the bucket name of \nzappatest-code\n in the 'US Standard' region.  This bucket will be used by zappa as a mechanism to upload your project into the lambda environment.  Thus it will generally be empty except during the brief time you are deploying the project.\n\n\nCreate an IAM User with API keys\n   Easier said than done.  The quick and easy way of doing this is to create a user with a policy that allows a very broad set of permissions.  However, this is not great from a security perspective. There is an \nongoing discussion\n about the exact set of permissions needed.\n\n\n\n\nNow we need to allow scripts and local programs to get the credentials created above.  You have some options for this:\n\n\nSetup Local Account Credentials\n\u00b6\n\n\n\n\n\n\nSet \nenvironment variables\n\n\nThis is very easy but must be done for each bash console you are using.\n\n\nexport AWS_ACCESS_KEY_ID=<your key here>\nexport AWS_SECRET_ACCESS_KEY=<your secret access key here>\n\n\n\n\n\n\n\n\nCreate a local credentials file (\n~/.aws/credentials\n on Linux, or OS X)\n\n\nProbably a better long term solution since you can store multiple \n sets of keys for different environments using profiles.  In addition, you can provide multiple profiles that provides some isolation between AWS accounts and/or roles.  The alternate profile example shown below is called 'zappa'\n\n\n[default]\n\n\naws_access_key_id\n \n=\n \nyour_access_key_id\n\n\naws_secret_access_key\n \n=\n \nyour_secret_access_key\n\n\n\n[zappa]\n\n\naws_access_key_id\n \n=\n \nyour_access_key_id_specific_to_zappa\n\n\naws_secret_access_key\n \n=\n \nyour_secret_access_key_specific_to_zappa\n\n\n\n\n\nSince you have multiple profiles, it is recommended that you use an environment variable to distinguish which profile is desired to be active.  Shown here is an example of using the 'zappa' profile:\n\n\nexport\n \nAWS_PROFILE\n=\nzappa\n\n\n\n\n\n\n\n\nUseful links for Windows or more information:\n\u00b6\n\n\n\n\nhttp://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-credentials.html\n\n\nhttp://boto3.readthedocs.io/en/latest/guide/configuration.html#configuring-credentials",
            "title": "Managing Credentials"
        },
        {
            "location": "/aws_credentials/#managing-aws-credentials",
            "text": "",
            "title": "Managing AWS Credentials"
        },
        {
            "location": "/aws_credentials/#getting-started-with-aws-and-zappa",
            "text": "Details in this section are light because this information is documented well elsewhere on the web.   Create AWS Account if you haven't already  Create an S3 bucket. \n   For purposes of this walkthrough I have used the bucket name of  zappatest-code  in the 'US Standard' region.  This bucket will be used by zappa as a mechanism to upload your project into the lambda environment.  Thus it will generally be empty except during the brief time you are deploying the project.  Create an IAM User with API keys\n   Easier said than done.  The quick and easy way of doing this is to create a user with a policy that allows a very broad set of permissions.  However, this is not great from a security perspective. There is an  ongoing discussion  about the exact set of permissions needed.   Now we need to allow scripts and local programs to get the credentials created above.  You have some options for this:",
            "title": "Getting Started with AWS and Zappa"
        },
        {
            "location": "/aws_credentials/#setup-local-account-credentials",
            "text": "Set  environment variables  This is very easy but must be done for each bash console you are using.  export AWS_ACCESS_KEY_ID=<your key here>\nexport AWS_SECRET_ACCESS_KEY=<your secret access key here>    Create a local credentials file ( ~/.aws/credentials  on Linux, or OS X)  Probably a better long term solution since you can store multiple \n sets of keys for different environments using profiles.  In addition, you can provide multiple profiles that provides some isolation between AWS accounts and/or roles.  The alternate profile example shown below is called 'zappa'  [default]  aws_access_key_id   =   your_access_key_id  aws_secret_access_key   =   your_secret_access_key  [zappa]  aws_access_key_id   =   your_access_key_id_specific_to_zappa  aws_secret_access_key   =   your_secret_access_key_specific_to_zappa   Since you have multiple profiles, it is recommended that you use an environment variable to distinguish which profile is desired to be active.  Shown here is an example of using the 'zappa' profile:  export   AWS_PROFILE = zappa",
            "title": "Setup Local Account Credentials"
        },
        {
            "location": "/aws_credentials/#useful-links-for-windows-or-more-information",
            "text": "http://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-credentials.html  http://boto3.readthedocs.io/en/latest/guide/configuration.html#configuring-credentials",
            "title": "Useful links for Windows or more information:"
        },
        {
            "location": "/aws_network_primer/",
            "text": "A Brief Primer on AWS VPC Networking (and how Lambda functions relate)\n\u00b6\n\n\nConfused on how to create your network environment in AWS?  Don't worry - you're in good company.  Configuring and using AWS VPC networking \nis powerful, but complex.  \n\n\nThis document attempts to create a \nmental model\n to help readers understand the concepts and thus \nallow more reasonable decisions to be made.  I find it helpful to draw analogies to traditional network setup within a company to assist building\nthe mental model.  \n\n\nIf you're already familiar with AWS VPC, you can skip to \nhow AWS Lambda interacts with VPC\n\n\nThis document takes the general approach:\n\n\n\n\nFirst, discuss the concepts and talk about how they relate\n\n\nLink to good tutorials on how to create VPCs once the reader has a good understanding\n\n\n\n\nWe won't go into exacting detail on how to do each step since there are many good tutorials already on the Interwebs.\n\n\nFirst there is a VPC\n\u00b6\n\n\nBack in the bad old days, networks were created by plugging a bunch of network cables into network hardware and were statically configured.\n\nIn AWS, the amazing thing is that you define the network dynamically by providing parameters to Amazon as a form of \nSoftware Defined Networking\n.  So Amazon has hardware in their datacenters but present to users a very dynamic environment is nearly indistinguishable from an old-school network.\n\n\nThe first thing most users do is define the overall boundaries of their network using a Virtual Private Cloud or VPC.  This is roughly akin to\ndrawing a circle around your infrastructure.  The old-school equivalent would be to define your company's internal network space.  This network\nspace would allow your various departments to have computers that belong to this space.  Old-school companies might have several floors and departments\nso it would have to be big enough to house all these computers.  Additionally, you don't want external hackers have unfettered access to your \ninternal databases and accounting systems so most companies pick \nPrivate Network Space\n to\nisolate the big bad Internet from the company network.\n\n\nYou do this in AWS VPC by defining the IP network space of the total possible IP addresses that \ncould\n live in the VPC.  Now you may not have \nlots and lots of computers you'll need to put into your VPC, but fortunately you are not charged by Amazon on how many IP addresses\nyou have reserved, so you can err on the side of having a bit of room.\n\n\nThe private network ranges available are defined by an Internet 'standard' called \nRFC 1918\n.  The private network spaces for IPv4 are:\n\n\n\n\n10.0.0.0 - 10.255.255.255\n (10/8 prefix)\n\n\n172.16.0.0 - 172.31.255.255\n (172.16/12 prefix)\n\n\n192.168.0.0 - 192.168.255.255\n (192.168/16 prefix)\n\n\n\n\nAWS restricts creation of VPCs with a /16 CIDR mask which limits you to a VPC with 65,536 IP addresses (which is pretty big).  This is probably a little excessive for your first VPC, so why not start with something like 10.0.0.0/20 which gives you about 4 thousand IP addresses?  Later we'll keep dividing this network space so this is a good start.\n\n\nNow create your subnets\n\u00b6\n\n\nOk, so now we've got a range of IP addresses that we can use and a potential route to the Internet.  Using our old-school network analogy,\nwhat a typical network engineer would do next is subdivide the network into chunks.  And the more technical term of a chunk of the \nnetwork is a \nsubnet\n.  In physical world, some possible reasons do this are:\n\n\n\n\nIf a building has multiple floors, maybe one subnet per floor\n\n\nMaybe divide a subnet for each business department (e.g. HR subnet, IT subnet, Software Development subnet)\n\n\nSubnet based on functionality: one subnet for the phone system, one subnet for desktop computers, one subnet for web servers\n\n\n\n\nIn the VPC world, the only real reasons needed to divide up the VPC is for functionality and security.  A very common example\nwould be if you want web application servers in a public subnet and database servers in a private subnet.  In this configuration,\nusers on the Internet can point their browsers at your website but cannot directly access your database servers.  And you can\ncreate a special connection from your public subnet to your private subnet so that only the web application servers can connect\nto the database servers.  In this way, you lessen the chances that bad actors can attack your database by restricting direct access.\n\n\nNotes on subnet calculations\n\u00b6\n\n\nThere are a lot of complex rules for how big and location of the subnets  within the VPC, but there are two important constraints:\n\n\n\n\nThe size of the subnets must be a power of 2 - which approximately results in the number of IP addresses assigned to the subnet \n  (e.g. 2, 4, 8, 16, 32, 64, 128, 256, etc IP addresses available)\n\n\nThe subnets must be contiguous - which means the available IP addresses in the chunk must be sequential\n\n\n\n\nBy far, the easiest way to visualize this system is to use a tool that handles all the complications for you.  I highly recommend\nthe \nSpiceworks Subnet Calculator\n\n\nBut there's more\n\u00b6\n\n\nJust a few more notes about subnets created within your VPC.  First, each subnet exists in one availability zone within one AWS Region.\nFor your experimentation, this should not be a big deal.  But once you have a production system that is designed to be highly-available,\nthen you will probably have to eventually create multiple subnets that are doing the same function (e.g. hosting a database or application\nservers) to guard against the case when one availability zone is having troubles, your infrastructure still is up. See the \nAWS documentation for more info on \nRegions and Availability Zones\n.\n\n\nEach subnet has a built-in \nAccess Control List (ACL)\n.  This will let\nyou control inbound and outbound network traffic at the TCP/IP level.  Thus you can create rules that allow or restrict network traffic\nthat applies to the entire subnet.  This is an important distinction from a more robust firewall or security device: regardless of how\nmany servers you may have in your subnet, they all share these rules.  So if a particular external IP address is permitted in the ACL,\nthen all servers could generate traffic to that external IP address.  If more fine-grained control per resource\nis necessary, see \nVPC Security Groups\n.\n\n\nLastly, there is a \nlimit of 200 of subnets per VPC\n\n imposed by AWS at the time of this writing.\n\n\nHooking things up: route tables\n\u00b6\n\n\nUsually you'll want various subnets to communicate -- and by default, AWS allows all traffic to flow from any subnet to any another.  So that's\nconvenient but usually we want a better security posture.\n\n\nAWS VPC uses route tables to figure out how network traffic should be shuffled around.  A route table is a configurable object within the AWS\nconsole.  When you create a new VPC, the AWS system will automatically create a route table for you and assign it to your VPC.  A VPC is \nalways assigned to a route table; and that assigned route table is the 'main' route table.  But there is no star, no icon, or anything special\nin the AWS console that this is a 'main' route table -- merely the fact that when you click on the VPC, only one route table will be listed there.\nAnd you cannot change a VPC 'main' route table.\n\n\nSo this 'main' route table has some special properties.  Turns out that each subnet is also assigned a route table.  And if you don't explicitly\nassign a route table on creation of the subnet, it gloms onto whatever the current 'main' route table is. So the 'main' route table is\nused as a default for all subnets that aren't assigned a specific route table.  Thus all the subnets will share a single route table by \ndefault unless you take action.  By taking action, I mean you can assign a different route table to a subnet.\n\n\nWhy do we need multiple route tables?\n\u00b6\n\n\nThe question you may be asking yourself is why do we even need more than one route table?  The most common usage is to increase security by \nspecializing the subnets.  Since we know all the subnets are really just networks, there is no functional difference until we modify how\nnetwork traffic flows between the subnets.  Back to the example of web app servers and databases.  We can put all the web app server \ninstances in Subnet A and the database server instances in Subnet B.  But then by restricting the routes so that Subnet B can only talk\nto Subnet A, we can consider Subnet B as 'private'.  Again, there is no sticker, no emjoi, nor icon in the AWS console that designates \nSubnet B as 'private' except for the fact that we've associated a restricted route table.  Conversely, we can allow Internet traffic to go\nout of Subnet A by assigning a different route table; thereby creating a 'public' subnet in name only.\n\n\nTalking to the Internet\n\u00b6\n\n\nUsually, most applications will need to communicate with the Internet; either taking incoming network connections \nor retrieving information. In AWS you connect your VPC to the Internet using \n\nInternet Gateways\n. \nThis is a special resource that is assigned to a VPC that allows network traffic to come in and out of your VPC.\nInternet Gateways are free to AWS users and can be created easily by using the VPC creation wizard or after the fact.\nAn Internet Gateway can only be attached to a single VPC at a time and a VPC can only have up to one IG attached at a time.\n\nBut VPCs do not have to have Internet Gateways attached.\n\n\nBut creating an Internet Gateway and then associating it with a VPC is only the preparation work.  In order to actually\nenable traffic into and out of a subnet, you must have a route associated with the subnet that connects the Internet Gateway.\nSo for the subnet you wish to designate as 'public' you must add a route:\n\n\n\n\nDestination: \n0.0.0.0/0\n - This is a special indicator to the route table that works as a 'catch-all' for any network\ntraffic destination not recognized\n\n\nTarget: \nigw-name-of-your-ig\n - Use the AWS console identifer of the IG associated with the current VPC\n\n\n\n\nAnd presto!  You can now consider any subnets associated with this route table as 'public' subnets.  Any EC2 instance\nthat is assigned an Elastic IP can now send and receive traffic from the Internet.  Any EC2 instances without an\nElastic IP will not be permitted to send and receive traffic.\n\n\nOther AWS Services\n\u00b6\n\n\nNote that many of the AWS services are not associated with any VPC and thus can be considered accessible via Internet-only.\nFor example, if your application living in the VPC needs to connect to the AWS Simple Queue Service (SQS), you will have to \nenable Internet access via an Internet Gateway as described above.\nAny AWS service that does not have a \nVPC endpoint\n capability is considered Internet only.\nAt the time of this writing, only the following services have \nVPC endpoints\n\n (and thus would not need an Internet Gateway):\n\n\n\n\nS3\n\n\nDynamoDB\n\n\nElasticSearch\n\n\n\n\nIn addition, some services like AWS RDS and ElastiCache can be assigned to VPC subnets \nand thus are natively accessible within a VPC.  These services effectively provide fully \nmanaged EC2 instances that provide the services and thus can be assigned to one or more VPC subnets.\n\n\nConnectivity from private subnets\n\u00b6\n\n\nOften, resources in 'private' subnets will have to communicate to the Internet.  Usually, this network\ntraffic is initiated from within the subnet and only return traffic is allowed.  If outside network traffic were permitted\nto initiate communication to 'private' subnet servers, it would expose a potential security risk and thus is not allowed.\n\n\nThe mechanism to allow servers and resources within a 'private' subnet to have outbound communication is to \nhave the network traffic sent to a special resource called a NAT device.  NAT stands for \nNetwork Address Translation\n \nand is generally used to hide a number of private servers or resources from any outside network.  This NAT device\nmust live in a 'public' subnet with access to the Internet since it acts as a middle-man for the network traffic: \nthe private resources network traffics gets sent from the 'private' subnet to the 'public' subnet and the NAT\ndevice passes along the data as if the NAT device was initiating the connection.\n\n\nEnabling a server or resource in a 'private' subnet to communicate to the Internet consists of:\n\n\n\n\nCreate the NAT Device (more on that below) in a 'public' subnet\n\n\nEdit the route table assigned to the 'private' subnet to have a new route:\n\n\nDestination: \n0.0.0.0/0\n - This is a special indicator to the route table that works as a 'catch-all' for any network\ntraffic destination not recognized\n\n\nTarget: \nnat-name-of-your-nat-device\n - Use the AWS console identifer of the NAT device in the 'public' subnet\n\n\n\n\nNote this will enable any server or resource in the 'private' subnet to use the NAT device for outbound communication.\n\n\nTypes of NAT Devices\n\u00b6\n\n\nNAT Devices are not free and the cost to the AWS account depends on the type of device.  To assist\nthe decision process, AWS has provided a guide on common types:\n\n\n\n\nNAT Gateways\n - these\n  are AWS-managed instances that are easy to spin-up and have minimal configuration.  This is the\n  easiest option for getting started.\n\n\nNAT Instances\n - these are based on an existing AWS AMI that creates an EC2 instance that you can further customize.\n\n\n\n\nAWS even provides a \ncomparison chart\n\nto help you decide.\n\n\nOther AWS Services\n\u00b6\n\n\nRecall that many AWS services are not accessible directly within a VPC.  So if your applications\ndepend on other AWS services such as SNS, SQS, SES, and so forth, a NAT device will be required\nat additional cost.\n\n\nAdvanced Topics\n\u00b6\n\n\nThere are many, many advanced configurations available with \nVPC\n:\n\n\n\n\nDNS\n\n\nVPN Connections\n\n\nDHCP\n\n\nVPC Peering\n\n\n\n\nBut none of these are required to get a basic VPC setup rolling, so they are left as an exercise for\nthe reader.\n\n\nNext Steps\n\u00b6\n\n\nI hope you have a solid mental model of the basic building block of AWS VPC and thus can understand the relationships\nbetween the myriad of concepts presented in the AWS documentation.  With this mental model, I recommend reading \ncarefully through the well-documented \nVPC Scenarios\n\n on the AWS documentation site.\n\n\nHow Lambda Works with VPC\n\u00b6\n\n\nNow that we've covered the basics with typical resources within a VPC, it's worth discussing\nhow AWS Lambda interacts with a VPC.  While it seems to be fairly straightforward, there is a surprise feature\nwith Lambda that bypasses the entire network model.\n\n\nMultiple Subnets allowed\n\u00b6\n\n\nWhen creating a Lambda function, you can assign zero, one, or multiple subnets.  If no subnets are specified, then the\nLambda function is considered external to all VPCs in the account.\n\n\nThe reason for multiple subnets would be mostly for robustness.  When a Lambda container is started, it must be assigned an\nIP address.  It will select the IP address from one of the assigned subnets.  The exact method for selection is not documented.\nThe Lambda container will run, then release the IP address.  But if you have many Lambda containers running, there is a \nrisk of using up all the available IP addresses.  In addition, since subnets are assigned a single availability zone (AZ),\nby associating subnets in a few different AZ, you can increase the chances that your Lambda function will continue to\nrun even if one AZ is having troubles.\n\n\nThe downside to assigning multiple subnets is you must ensure the routing rules are identical for both subnets otherwise\nyou could create a situation where Lambda containers may behave differently based on the dynamically assigned subnet.  This\nwould be very difficult to troubleshoot.\n\n\nGeneral Behavior and Internet Access\n\u00b6\n\n\nWhen instantiated, Lambda functions are dynamically assigned an \n\nElastic Network Interface (ENI)\n. \nThis ENI provides the Lambda function the capability to use the VPC subnet network - like someone\nplugged in a virtual ethernet plug. \nThe ENI conforms to the typical network address for the VPC and subnet to which the Lambda function is assigned. \nThat is, if the Lambda function is instantiated in a public subnet, the ENI will be a public IP; and if the Lambda function is instantiated in a private subnet, the ENI will be a private IP. \nAnd if there are other resources like an EC2 instance running in the same subnet, the Lambda function\ncan interact with the EC2 instance just fine.\n\n\nHowever, there are restrictions placed upon Lambda functions when attempting to access the Internet.\nNo Lambda functions running within a VPC will be able to access\nthe Internet directly.  Even if the Lambda function is assigned to a 'public' subnet \nwith access to an Internet Gateway, the Lambda function \nwill not be able to leverage the Internet Gateway.  As per \nAWS Documentation\n if you want Lambda functions to access the Internet,\nyou must assign those Lambda functions to a 'private' subnet and leverage a NAT device as described above.  This includes\nthe scenario when the Lambda functions needs to interact with other AWS services that are considered only available\nvia the Internet.\n\n\nAside from that, Lambda functions that do not require Internet access can happily operate in any of your VPC subnets.\n\n\nBypassing your VPC setup\n\u00b6\n\n\nIt is important to remember that Lambda functions are a separate and distinct AWS service from VPC.  VPC merely defines\nthe IP network space in which a Lambda function will operate.  The heart of the Lambda service could be considered\nthe ability to be triggered from an event and optionally respond to the event.  These events come from various sources\nand services within the AWS fabric.  It can be described as an 'event fabric' consisting of large number \nof Lambda functions listening for events, processing them, and responding.\n\nIn fact, this method of communication is completely separate and distinct from network traffic.  It has its own\nsecurity model and retry mechanism -- very much like a point-to-point network.  \n\n\nThe implications of leveraging this 'event network' is that it can be used to effectively 'tunnel' across separate\nand distinct VPC and subnets.  It takes a little bit of work but consider:\n\n\n\n\nLambda X deployed outside of any VPC in your account\n\n\nLambda Y deployed in a 'private' subnet within the VPC\n\n\nRDS instance in 'private' subnet within the VPC\n\n\nAPI Gateway deployed in your account (also outside any VPC)\n\n\n\n\nIf properly configured the following sequence could happen:\n\n\n\n\nAPI Gateway creates an event for Lambda X\n\n\nLambda X gets the event, then creates an event for Lambda Y\n\n\nLambda Y gets the event, then makes a query to the RDS database via traditional network mechanisms\n\n\nLambda Y returns the information to Lambda X\n\n\nLambda X returns the information to API Gateway\n\n\n\n\nNow this is clearly a contrived example as API Gateway could actually invoke Lambda Y directly.  But it is meant\nto illustrate that with some limitations, there are ways to get around the VPC constructs.\nA more useful scenario would be to have Lambda Y invoke the Lambda X function to gain access to 'Internet only'\nAWS services such as SQS without the need for a NAT Device.  There are many complications with this strategy:\n\n\n\n\nYou now have to juggle two different authorization schemes - Lambda and traditional access methods\n\n\nYou have doubled the number of Lambda function invocations which could lead to additional charges\n\n\nYou have a execution limit on both the number simultaneous of Lambda functions and length of time allowed\n  per invocation\n\n\nYou have to create and manage the event chain manually\n\n\n\n\nIn my opinion, it is important to know this functionality exists, but in almost all cases, it would be more\ncost effective to leverage one of the NAT Device strategies outlined above.\n\n\nAt the time of this writing, the following events can \n\ngenerate events for Lambda functions\n:\n\n\n\n\nAmazon S3\n\n\nAmazon DynamoDB\n\n\nAmazon Kinesis Streams\n\n\nAmazon Simple Notification Service\n\n\nAmazon Simple Email Service\n\n\nAmazon Cognito\n\n\nAWS CloudFormation\n\n\nAmazon CloudWatch Logs\n\n\nAmazon CloudWatch Events\n\n\nAWS CodeCommit\n\n\nScheduled Events (powered by Amazon CloudWatch Events)\n\n\nAWS Config\n\n\nAmazon Alexa\n\n\nAmazon Lex\n\n\nAmazon API Gateway\n\n\nAmazon Simple Queue Service\n\n\n\n\nFinal considerations\n\u00b6\n\n\nIt takes some planning when planning to deploy a Zappa environment and to determine if VPC is required or necessary.\nIn the \nnext section\n, we discuss some concrete options for the walkthrough.",
            "title": "Primer on AWS VPC Networking"
        },
        {
            "location": "/aws_network_primer/#a-brief-primer-on-aws-vpc-networking-and-how-lambda-functions-relate",
            "text": "Confused on how to create your network environment in AWS?  Don't worry - you're in good company.  Configuring and using AWS VPC networking \nis powerful, but complex.    This document attempts to create a  mental model  to help readers understand the concepts and thus \nallow more reasonable decisions to be made.  I find it helpful to draw analogies to traditional network setup within a company to assist building\nthe mental model.    If you're already familiar with AWS VPC, you can skip to  how AWS Lambda interacts with VPC  This document takes the general approach:   First, discuss the concepts and talk about how they relate  Link to good tutorials on how to create VPCs once the reader has a good understanding   We won't go into exacting detail on how to do each step since there are many good tutorials already on the Interwebs.",
            "title": "A Brief Primer on AWS VPC Networking (and how Lambda functions relate)"
        },
        {
            "location": "/aws_network_primer/#first-there-is-a-vpc",
            "text": "Back in the bad old days, networks were created by plugging a bunch of network cables into network hardware and were statically configured. \nIn AWS, the amazing thing is that you define the network dynamically by providing parameters to Amazon as a form of  Software Defined Networking .  So Amazon has hardware in their datacenters but present to users a very dynamic environment is nearly indistinguishable from an old-school network.  The first thing most users do is define the overall boundaries of their network using a Virtual Private Cloud or VPC.  This is roughly akin to\ndrawing a circle around your infrastructure.  The old-school equivalent would be to define your company's internal network space.  This network\nspace would allow your various departments to have computers that belong to this space.  Old-school companies might have several floors and departments\nso it would have to be big enough to house all these computers.  Additionally, you don't want external hackers have unfettered access to your \ninternal databases and accounting systems so most companies pick  Private Network Space  to\nisolate the big bad Internet from the company network.  You do this in AWS VPC by defining the IP network space of the total possible IP addresses that  could  live in the VPC.  Now you may not have \nlots and lots of computers you'll need to put into your VPC, but fortunately you are not charged by Amazon on how many IP addresses\nyou have reserved, so you can err on the side of having a bit of room.  The private network ranges available are defined by an Internet 'standard' called  RFC 1918 .  The private network spaces for IPv4 are:   10.0.0.0 - 10.255.255.255  (10/8 prefix)  172.16.0.0 - 172.31.255.255  (172.16/12 prefix)  192.168.0.0 - 192.168.255.255  (192.168/16 prefix)   AWS restricts creation of VPCs with a /16 CIDR mask which limits you to a VPC with 65,536 IP addresses (which is pretty big).  This is probably a little excessive for your first VPC, so why not start with something like 10.0.0.0/20 which gives you about 4 thousand IP addresses?  Later we'll keep dividing this network space so this is a good start.",
            "title": "First there is a VPC"
        },
        {
            "location": "/aws_network_primer/#now-create-your-subnets",
            "text": "Ok, so now we've got a range of IP addresses that we can use and a potential route to the Internet.  Using our old-school network analogy,\nwhat a typical network engineer would do next is subdivide the network into chunks.  And the more technical term of a chunk of the \nnetwork is a  subnet .  In physical world, some possible reasons do this are:   If a building has multiple floors, maybe one subnet per floor  Maybe divide a subnet for each business department (e.g. HR subnet, IT subnet, Software Development subnet)  Subnet based on functionality: one subnet for the phone system, one subnet for desktop computers, one subnet for web servers   In the VPC world, the only real reasons needed to divide up the VPC is for functionality and security.  A very common example\nwould be if you want web application servers in a public subnet and database servers in a private subnet.  In this configuration,\nusers on the Internet can point their browsers at your website but cannot directly access your database servers.  And you can\ncreate a special connection from your public subnet to your private subnet so that only the web application servers can connect\nto the database servers.  In this way, you lessen the chances that bad actors can attack your database by restricting direct access.",
            "title": "Now create your subnets"
        },
        {
            "location": "/aws_network_primer/#notes-on-subnet-calculations",
            "text": "There are a lot of complex rules for how big and location of the subnets  within the VPC, but there are two important constraints:   The size of the subnets must be a power of 2 - which approximately results in the number of IP addresses assigned to the subnet \n  (e.g. 2, 4, 8, 16, 32, 64, 128, 256, etc IP addresses available)  The subnets must be contiguous - which means the available IP addresses in the chunk must be sequential   By far, the easiest way to visualize this system is to use a tool that handles all the complications for you.  I highly recommend\nthe  Spiceworks Subnet Calculator",
            "title": "Notes on subnet calculations"
        },
        {
            "location": "/aws_network_primer/#but-theres-more",
            "text": "Just a few more notes about subnets created within your VPC.  First, each subnet exists in one availability zone within one AWS Region.\nFor your experimentation, this should not be a big deal.  But once you have a production system that is designed to be highly-available,\nthen you will probably have to eventually create multiple subnets that are doing the same function (e.g. hosting a database or application\nservers) to guard against the case when one availability zone is having troubles, your infrastructure still is up. See the \nAWS documentation for more info on  Regions and Availability Zones .  Each subnet has a built-in  Access Control List (ACL) .  This will let\nyou control inbound and outbound network traffic at the TCP/IP level.  Thus you can create rules that allow or restrict network traffic\nthat applies to the entire subnet.  This is an important distinction from a more robust firewall or security device: regardless of how\nmany servers you may have in your subnet, they all share these rules.  So if a particular external IP address is permitted in the ACL,\nthen all servers could generate traffic to that external IP address.  If more fine-grained control per resource\nis necessary, see  VPC Security Groups .  Lastly, there is a  limit of 200 of subnets per VPC \n imposed by AWS at the time of this writing.",
            "title": "But there's more"
        },
        {
            "location": "/aws_network_primer/#hooking-things-up-route-tables",
            "text": "Usually you'll want various subnets to communicate -- and by default, AWS allows all traffic to flow from any subnet to any another.  So that's\nconvenient but usually we want a better security posture.  AWS VPC uses route tables to figure out how network traffic should be shuffled around.  A route table is a configurable object within the AWS\nconsole.  When you create a new VPC, the AWS system will automatically create a route table for you and assign it to your VPC.  A VPC is \nalways assigned to a route table; and that assigned route table is the 'main' route table.  But there is no star, no icon, or anything special\nin the AWS console that this is a 'main' route table -- merely the fact that when you click on the VPC, only one route table will be listed there.\nAnd you cannot change a VPC 'main' route table.  So this 'main' route table has some special properties.  Turns out that each subnet is also assigned a route table.  And if you don't explicitly\nassign a route table on creation of the subnet, it gloms onto whatever the current 'main' route table is. So the 'main' route table is\nused as a default for all subnets that aren't assigned a specific route table.  Thus all the subnets will share a single route table by \ndefault unless you take action.  By taking action, I mean you can assign a different route table to a subnet.",
            "title": "Hooking things up: route tables"
        },
        {
            "location": "/aws_network_primer/#why-do-we-need-multiple-route-tables",
            "text": "The question you may be asking yourself is why do we even need more than one route table?  The most common usage is to increase security by \nspecializing the subnets.  Since we know all the subnets are really just networks, there is no functional difference until we modify how\nnetwork traffic flows between the subnets.  Back to the example of web app servers and databases.  We can put all the web app server \ninstances in Subnet A and the database server instances in Subnet B.  But then by restricting the routes so that Subnet B can only talk\nto Subnet A, we can consider Subnet B as 'private'.  Again, there is no sticker, no emjoi, nor icon in the AWS console that designates \nSubnet B as 'private' except for the fact that we've associated a restricted route table.  Conversely, we can allow Internet traffic to go\nout of Subnet A by assigning a different route table; thereby creating a 'public' subnet in name only.",
            "title": "Why do we need multiple route tables?"
        },
        {
            "location": "/aws_network_primer/#talking-to-the-internet",
            "text": "Usually, most applications will need to communicate with the Internet; either taking incoming network connections \nor retrieving information. In AWS you connect your VPC to the Internet using  Internet Gateways . \nThis is a special resource that is assigned to a VPC that allows network traffic to come in and out of your VPC.\nInternet Gateways are free to AWS users and can be created easily by using the VPC creation wizard or after the fact.\nAn Internet Gateway can only be attached to a single VPC at a time and a VPC can only have up to one IG attached at a time. \nBut VPCs do not have to have Internet Gateways attached.  But creating an Internet Gateway and then associating it with a VPC is only the preparation work.  In order to actually\nenable traffic into and out of a subnet, you must have a route associated with the subnet that connects the Internet Gateway.\nSo for the subnet you wish to designate as 'public' you must add a route:   Destination:  0.0.0.0/0  - This is a special indicator to the route table that works as a 'catch-all' for any network\ntraffic destination not recognized  Target:  igw-name-of-your-ig  - Use the AWS console identifer of the IG associated with the current VPC   And presto!  You can now consider any subnets associated with this route table as 'public' subnets.  Any EC2 instance\nthat is assigned an Elastic IP can now send and receive traffic from the Internet.  Any EC2 instances without an\nElastic IP will not be permitted to send and receive traffic.",
            "title": "Talking to the Internet"
        },
        {
            "location": "/aws_network_primer/#other-aws-services",
            "text": "Note that many of the AWS services are not associated with any VPC and thus can be considered accessible via Internet-only.\nFor example, if your application living in the VPC needs to connect to the AWS Simple Queue Service (SQS), you will have to \nenable Internet access via an Internet Gateway as described above.\nAny AWS service that does not have a  VPC endpoint  capability is considered Internet only.\nAt the time of this writing, only the following services have  VPC endpoints \n (and thus would not need an Internet Gateway):   S3  DynamoDB  ElasticSearch   In addition, some services like AWS RDS and ElastiCache can be assigned to VPC subnets \nand thus are natively accessible within a VPC.  These services effectively provide fully \nmanaged EC2 instances that provide the services and thus can be assigned to one or more VPC subnets.",
            "title": "Other AWS Services"
        },
        {
            "location": "/aws_network_primer/#connectivity-from-private-subnets",
            "text": "Often, resources in 'private' subnets will have to communicate to the Internet.  Usually, this network\ntraffic is initiated from within the subnet and only return traffic is allowed.  If outside network traffic were permitted\nto initiate communication to 'private' subnet servers, it would expose a potential security risk and thus is not allowed.  The mechanism to allow servers and resources within a 'private' subnet to have outbound communication is to \nhave the network traffic sent to a special resource called a NAT device.  NAT stands for  Network Address Translation  \nand is generally used to hide a number of private servers or resources from any outside network.  This NAT device\nmust live in a 'public' subnet with access to the Internet since it acts as a middle-man for the network traffic: \nthe private resources network traffics gets sent from the 'private' subnet to the 'public' subnet and the NAT\ndevice passes along the data as if the NAT device was initiating the connection.  Enabling a server or resource in a 'private' subnet to communicate to the Internet consists of:   Create the NAT Device (more on that below) in a 'public' subnet  Edit the route table assigned to the 'private' subnet to have a new route:  Destination:  0.0.0.0/0  - This is a special indicator to the route table that works as a 'catch-all' for any network\ntraffic destination not recognized  Target:  nat-name-of-your-nat-device  - Use the AWS console identifer of the NAT device in the 'public' subnet   Note this will enable any server or resource in the 'private' subnet to use the NAT device for outbound communication.",
            "title": "Connectivity from private subnets"
        },
        {
            "location": "/aws_network_primer/#types-of-nat-devices",
            "text": "NAT Devices are not free and the cost to the AWS account depends on the type of device.  To assist\nthe decision process, AWS has provided a guide on common types:   NAT Gateways  - these\n  are AWS-managed instances that are easy to spin-up and have minimal configuration.  This is the\n  easiest option for getting started.  NAT Instances  - these are based on an existing AWS AMI that creates an EC2 instance that you can further customize.   AWS even provides a  comparison chart \nto help you decide.",
            "title": "Types of NAT Devices"
        },
        {
            "location": "/aws_network_primer/#other-aws-services_1",
            "text": "Recall that many AWS services are not accessible directly within a VPC.  So if your applications\ndepend on other AWS services such as SNS, SQS, SES, and so forth, a NAT device will be required\nat additional cost.",
            "title": "Other AWS Services"
        },
        {
            "location": "/aws_network_primer/#advanced-topics",
            "text": "There are many, many advanced configurations available with  VPC :   DNS  VPN Connections  DHCP  VPC Peering   But none of these are required to get a basic VPC setup rolling, so they are left as an exercise for\nthe reader.",
            "title": "Advanced Topics"
        },
        {
            "location": "/aws_network_primer/#next-steps",
            "text": "I hope you have a solid mental model of the basic building block of AWS VPC and thus can understand the relationships\nbetween the myriad of concepts presented in the AWS documentation.  With this mental model, I recommend reading \ncarefully through the well-documented  VPC Scenarios \n on the AWS documentation site.",
            "title": "Next Steps"
        },
        {
            "location": "/aws_network_primer/#how-lambda-works-with-vpc",
            "text": "Now that we've covered the basics with typical resources within a VPC, it's worth discussing\nhow AWS Lambda interacts with a VPC.  While it seems to be fairly straightforward, there is a surprise feature\nwith Lambda that bypasses the entire network model.",
            "title": "How Lambda Works with VPC"
        },
        {
            "location": "/aws_network_primer/#multiple-subnets-allowed",
            "text": "When creating a Lambda function, you can assign zero, one, or multiple subnets.  If no subnets are specified, then the\nLambda function is considered external to all VPCs in the account.  The reason for multiple subnets would be mostly for robustness.  When a Lambda container is started, it must be assigned an\nIP address.  It will select the IP address from one of the assigned subnets.  The exact method for selection is not documented.\nThe Lambda container will run, then release the IP address.  But if you have many Lambda containers running, there is a \nrisk of using up all the available IP addresses.  In addition, since subnets are assigned a single availability zone (AZ),\nby associating subnets in a few different AZ, you can increase the chances that your Lambda function will continue to\nrun even if one AZ is having troubles.  The downside to assigning multiple subnets is you must ensure the routing rules are identical for both subnets otherwise\nyou could create a situation where Lambda containers may behave differently based on the dynamically assigned subnet.  This\nwould be very difficult to troubleshoot.",
            "title": "Multiple Subnets allowed"
        },
        {
            "location": "/aws_network_primer/#general-behavior-and-internet-access",
            "text": "When instantiated, Lambda functions are dynamically assigned an  Elastic Network Interface (ENI) . \nThis ENI provides the Lambda function the capability to use the VPC subnet network - like someone\nplugged in a virtual ethernet plug. \nThe ENI conforms to the typical network address for the VPC and subnet to which the Lambda function is assigned. \nThat is, if the Lambda function is instantiated in a public subnet, the ENI will be a public IP; and if the Lambda function is instantiated in a private subnet, the ENI will be a private IP. \nAnd if there are other resources like an EC2 instance running in the same subnet, the Lambda function\ncan interact with the EC2 instance just fine.  However, there are restrictions placed upon Lambda functions when attempting to access the Internet.\nNo Lambda functions running within a VPC will be able to access\nthe Internet directly.  Even if the Lambda function is assigned to a 'public' subnet \nwith access to an Internet Gateway, the Lambda function \nwill not be able to leverage the Internet Gateway.  As per  AWS Documentation  if you want Lambda functions to access the Internet,\nyou must assign those Lambda functions to a 'private' subnet and leverage a NAT device as described above.  This includes\nthe scenario when the Lambda functions needs to interact with other AWS services that are considered only available\nvia the Internet.  Aside from that, Lambda functions that do not require Internet access can happily operate in any of your VPC subnets.",
            "title": "General Behavior and Internet Access"
        },
        {
            "location": "/aws_network_primer/#bypassing-your-vpc-setup",
            "text": "It is important to remember that Lambda functions are a separate and distinct AWS service from VPC.  VPC merely defines\nthe IP network space in which a Lambda function will operate.  The heart of the Lambda service could be considered\nthe ability to be triggered from an event and optionally respond to the event.  These events come from various sources\nand services within the AWS fabric.  It can be described as an 'event fabric' consisting of large number \nof Lambda functions listening for events, processing them, and responding. \nIn fact, this method of communication is completely separate and distinct from network traffic.  It has its own\nsecurity model and retry mechanism -- very much like a point-to-point network.    The implications of leveraging this 'event network' is that it can be used to effectively 'tunnel' across separate\nand distinct VPC and subnets.  It takes a little bit of work but consider:   Lambda X deployed outside of any VPC in your account  Lambda Y deployed in a 'private' subnet within the VPC  RDS instance in 'private' subnet within the VPC  API Gateway deployed in your account (also outside any VPC)   If properly configured the following sequence could happen:   API Gateway creates an event for Lambda X  Lambda X gets the event, then creates an event for Lambda Y  Lambda Y gets the event, then makes a query to the RDS database via traditional network mechanisms  Lambda Y returns the information to Lambda X  Lambda X returns the information to API Gateway   Now this is clearly a contrived example as API Gateway could actually invoke Lambda Y directly.  But it is meant\nto illustrate that with some limitations, there are ways to get around the VPC constructs.\nA more useful scenario would be to have Lambda Y invoke the Lambda X function to gain access to 'Internet only'\nAWS services such as SQS without the need for a NAT Device.  There are many complications with this strategy:   You now have to juggle two different authorization schemes - Lambda and traditional access methods  You have doubled the number of Lambda function invocations which could lead to additional charges  You have a execution limit on both the number simultaneous of Lambda functions and length of time allowed\n  per invocation  You have to create and manage the event chain manually   In my opinion, it is important to know this functionality exists, but in almost all cases, it would be more\ncost effective to leverage one of the NAT Device strategies outlined above.  At the time of this writing, the following events can  generate events for Lambda functions :   Amazon S3  Amazon DynamoDB  Amazon Kinesis Streams  Amazon Simple Notification Service  Amazon Simple Email Service  Amazon Cognito  AWS CloudFormation  Amazon CloudWatch Logs  Amazon CloudWatch Events  AWS CodeCommit  Scheduled Events (powered by Amazon CloudWatch Events)  AWS Config  Amazon Alexa  Amazon Lex  Amazon API Gateway  Amazon Simple Queue Service",
            "title": "Bypassing your VPC setup"
        },
        {
            "location": "/aws_network_primer/#final-considerations",
            "text": "It takes some planning when planning to deploy a Zappa environment and to determine if VPC is required or necessary.\nIn the  next section , we discuss some concrete options for the walkthrough.",
            "title": "Final considerations"
        },
        {
            "location": "/aws_network/",
            "text": "Adventures in Networking\n\u00b6\n\n\nPresumably, since you've read this far, you're interested in more than a simple Django powered API serving static images.  You probably want something that is more interactive with users and have advanced capabilities.  So this will require interacting with additional AWS services like RDS or DynamoDB for database services, SNS or SQS for task processing, and many more...\n\n\nWell, there is an important aspect about interacting with additional AWS services.  With pure AWS lambda, we have a fairly low attack surface, but when we introduce additional interactions over the network, we must consider information security risks and our network architecture.\n\n\n\n\nNote\n\n\nThis section assumes some familarity with the basic concepts within AWS VPC.  If you aren't comfortable with these concepts,\nthen head over to a \nPrimer on AWS VPC Networking\n\n\n\n\nA simple, but naive approach\n\u00b6\n\n\nA simple approach would be to create an AWS RDS instance that is open to the Internet and have your lambda Django project login directly.  While this may work, this approach is fraught with peril.  Numerous vulnerabilities could exist and credentials could be discovered by brute force.  Even without gaining access to your RDS, it is trivial to launch a denial-of-service attack to ensure your Django project has no database services.\n\n\nBasically, do not do this.\n\n\nThat's why Amazon has VPC\n\u00b6\n\n\nIntroducing AWS Virtual Private Cloud (VPC).  This service provides a way of segmenting Internet traffic from your other AWS services.  This feature is incredibly valuable to securing our project, so that bad guys have the smallest attack surface possible.  \n\n\nOnce a VPC is established, you can then subdivide the VPC into subnets.  Which are little non-overlapping IP chunks of the overall VPC.  Like dividing a pie into slices.\n\n\nIt is important to realize that you have almost entire control of your IP space when you define your VPC and associated subnets.  You are creating a Software Defined Network (SDN) and you have a lot of leeway.  With this freedom comes choices and important considerations on how you design your network.\n\n\nThe Essential VPC and Subnet Configuration\n\u00b6\n\n\nHere is the absolute minimum you must have to have a working Django site within a VPC:\n\n\n\n\nA VPC\n\n\nA subnet within the VPC\n\n\nA security group\n\n\nYour zappa project configured to use the subnet and security group\n\n\n\n\nThere are no need for routes, NAT gateways, Internet gateways, or even allowing inbound rules on the security group.  You will probably need many of these services eventually for a robust and full-featured application, but for illustration purposes now, we will keep it simple.\n\n\nYou may be asking yourself how will traffic from the Internet reach the lambda function. Well essentially the magic is in the API Gateway that zappa creates on deployment.  From the \nAPI Gateway FAQ\n:\n\n\n\n\nAmazon API Gateway endpoints are always public to the Internet. Proxy requests to backend operations also need to be publicly accessible on the Internet. However, you can generate a client-side SSL certificate in Amazon API Gateway to verify that requests to your backend systems were sent by API Gateway using the public key of the certificate.\n\n\n\n\nSo API Gateways act as an 'always-on' Internet facing service that sends the HTTP requests to the Zappa Lambda functions by creating \nLambda events.  These events can reach your zappa application even if deployed in a top-secret lockbox.  Note that in the above VPC setup the Lambda function is completely isolated from direct access to the Internet.  Thus the Lambda functions can't make outbound connections to anything: other servers, databases, S3, etc.\n\n\nLet's face it, there's no real point to going through the effort of doing this if the Lambda functions are really this isolated.\n\n\nBut\n the reason this information is important is because in order to leverage other AWS resources, \nthe VPC lays the foundation to do this securely.\n\n\nExtending the VPC\n\u00b6\n\n\nTo re-iterate: we don't need a VPC network to make the Zappa Lambda-powered Django site visible on the Internet.\n\nWe need to extend the VPC so that we can add more AWS services like:\n\n\n\n\nAdding an RDS database that is only accessible from our Lambda functions\n\n\nAdding an S3 bucket \n\n\nAllowing Lambda functions to communicate to traditional EC2 instances for long running processes\n\n\nAllowing the Lambda functions to access the Internet to hit an external API\n\n\nAbility to interact with SQS and/or SNS\n\n\n\n\nThe important thing is that we can enable all these scenarios in a secure manner.  There are too many scenarios to go into detail here, so we will provide some guidelines.\n\n\nNext we learn about the common patterns of VPC usage and try to identify usage options.\n\n\nOne warning before we go on with VPC subnet sizing and Lambda:  When Lambda functions fire, they must be assigned an IP address temporarily.  If you have a lot of Lambda functions firing, then you must have a lot of IP addresses available in your subnet.  You can learn more \nhere\n.\n\n\nVPC Patterns\n\u00b6\n\n\nWhile the actual VPC itself is very straightforward, the combination of subnets within the VPC can take many forms.  Additionally, you must consider how the subnets will communicate among themselves as well as how  they communicate with outside networks. For a more comprehensive list of options and other valuable information about VPCs, see this link: \nhttps://aws.amazon.com/answers/networking/aws-single-vpc-design/\n\n\nThe options are summarized here:\n\n\nVPC with a single Internet-Accessible subnet\n\u00b6\n\n\nThis pattern places your lambda functions, your RDS, and additional SNS/SQS services in a single subnet that is Internet accessible in your VPC.  In theory you could configure your security groups to ensure only lambda functions can hit your RDS.\n\n\nOne advantage of this setup is that you can setup your local machine to connect to your RDS without a \nbastion host\n.  Just restrict access based on IP.  \n\n\nImportant note - you will want to ensure careful inbound IP restrictions.  While it's great that you can connect to RDS with your SQL desktop client, you should setup a \nbastion host\n.\n\n\nAlso note that your zappa deployment \n\nwill not have outbound access to the Internet\n.\nIn order to do this you will have use a Public/Private setup.\n\n\n\n\nSome additional considerations from the \nAWS Documentation\n\n\n\n\n\n\nWe recommend that you avoid DNS resolution of public host names for your VPC. This can take several seconds to resolve, which adds several seconds of billable time on your request. For example, if your Lambda function accesses an Amazon RDS instance in your VPC, launch the instance with the no-publicly-accessible option.\n\n\n\n\n\n\nWhen you add VPC configuration to a Lambda function, it can only access resources in that VPC. If a Lambda function needs to access both VPC resources and the public Internet, the VPC needs to have a Network Address Translation (NAT) instance inside the VPC.\n\n\n\n\n\n\nWhen a Lambda function is configured to run within a VPC, it incurs an additional ENI start-up penalty. This means address resolution may be delayed when trying to connect to network resources.\n\n\n\n\n\n\n\n\nThis scenario is good for straightforward setups with a little work, but has significant limitations\n\n\nVPC with a Public subnet and Private subnet\n\u00b6\n\n\nArguably the most flexible and future-proof of all web application setups.  You have two subnets: one public and one private.  Your RDS and lambda functions reside in the private subnet, far away from bad guys, but also far away from your local machine.  In order to access the database from your system you will need a \nbastion host\n in the public subnet.  \n\n\nAnother advantage of this setup is that if you ever want to add additional EC2-based services that need to interact with the Internet you can do this very easily without compromising security.  \n\n\nGenerally this setup will require networking knowledge to setup the Internet gateway, bastion host, and NAT Gateway.\n\n\nThe upshot is this configuration is the most secure and most flexible for your growth but will be complex from a network standpoint\n\n\nOn-Premises and Internet-Accessible VPC\n\u00b6\n\n\nSame as the last configuration, but if you have an internal corporate network to connect, you can easily establish a connection to the private subnet without compromising security.\n\n\nIf you thought the last setup was complex, you better know what you are doing from a network standpoint.\n\n\nA good solution if you need to connect an internal corporate network\n\n\nVPC with an Internal-Only subnet\n\u00b6\n\n\nObviously a very special case of creating a Django app for internal use only with no desire to have it accessible by the Internet.\n\n\nThis is actually most secure.  Since you can access any of the resources from your desktop on the internal network.  Not bad for the paranoid or security conscious devops team.\n\n\nUseful for the simple environment if you have an existing secure network\n\n\nSubdividing the VPC\n\u00b6\n\n\nOnce you get a VPC selected you must create subnets within the VPC.  When defining a subnet, you just have to pick a non-overlapping segment of the ip range.  So if you have VPC that spans IP address 10.5.0.1 to 10.5.0.254, then you pick contiguous segments within this range.\n\n\nSee more details in \nPrimer on AWS VPC Networking\n\n\nExamples for Walkthroughs\n\u00b6\n\n\nFor the purposes of walkthroughs, we will leverage a simple VPC with a single subnet.  A single subnet will generally be enough to guide readers through the scenarios.\n\n\nWe have a VPC:\n\n\n\n\nid: vpc-9a9a1dfc\n\n\ncidr: 10.6.0.0/16\n\n\n\n\nWith subnet:\n\n\n\n\nid: subnet-f3446aba\n\n\ncidr: 10.6.1.0/24\n\n\n\n\nAnd security group:\n\n\n\n\nid: sg-13a5736f\n\n\ninbound rules: none\n\n\noutbound rules: all traffic\n\n\n\n\n\n\nTODO: Show example zappa configuration here\n\n\n\n\nNote on Redundancy\n\u00b6\n\n\nWhile these examples are all using a single subnet for clarity, in production you will want to create multiple subnets within the VPC all with different availability zones.  This ensures if there is a failure within a single availability zone, there are alternate paths.  \n\n\nThe general approach is to associate the Lambda functions with multiple subnets and the AWS resources with the same multiple subnets (e.g. RDS).",
            "title": "Adventures in Networking"
        },
        {
            "location": "/aws_network/#adventures-in-networking",
            "text": "Presumably, since you've read this far, you're interested in more than a simple Django powered API serving static images.  You probably want something that is more interactive with users and have advanced capabilities.  So this will require interacting with additional AWS services like RDS or DynamoDB for database services, SNS or SQS for task processing, and many more...  Well, there is an important aspect about interacting with additional AWS services.  With pure AWS lambda, we have a fairly low attack surface, but when we introduce additional interactions over the network, we must consider information security risks and our network architecture.   Note  This section assumes some familarity with the basic concepts within AWS VPC.  If you aren't comfortable with these concepts,\nthen head over to a  Primer on AWS VPC Networking",
            "title": "Adventures in Networking"
        },
        {
            "location": "/aws_network/#a-simple-but-naive-approach",
            "text": "A simple approach would be to create an AWS RDS instance that is open to the Internet and have your lambda Django project login directly.  While this may work, this approach is fraught with peril.  Numerous vulnerabilities could exist and credentials could be discovered by brute force.  Even without gaining access to your RDS, it is trivial to launch a denial-of-service attack to ensure your Django project has no database services.  Basically, do not do this.",
            "title": "A simple, but naive approach"
        },
        {
            "location": "/aws_network/#thats-why-amazon-has-vpc",
            "text": "Introducing AWS Virtual Private Cloud (VPC).  This service provides a way of segmenting Internet traffic from your other AWS services.  This feature is incredibly valuable to securing our project, so that bad guys have the smallest attack surface possible.    Once a VPC is established, you can then subdivide the VPC into subnets.  Which are little non-overlapping IP chunks of the overall VPC.  Like dividing a pie into slices.  It is important to realize that you have almost entire control of your IP space when you define your VPC and associated subnets.  You are creating a Software Defined Network (SDN) and you have a lot of leeway.  With this freedom comes choices and important considerations on how you design your network.",
            "title": "That's why Amazon has VPC"
        },
        {
            "location": "/aws_network/#the-essential-vpc-and-subnet-configuration",
            "text": "Here is the absolute minimum you must have to have a working Django site within a VPC:   A VPC  A subnet within the VPC  A security group  Your zappa project configured to use the subnet and security group   There are no need for routes, NAT gateways, Internet gateways, or even allowing inbound rules on the security group.  You will probably need many of these services eventually for a robust and full-featured application, but for illustration purposes now, we will keep it simple.  You may be asking yourself how will traffic from the Internet reach the lambda function. Well essentially the magic is in the API Gateway that zappa creates on deployment.  From the  API Gateway FAQ :   Amazon API Gateway endpoints are always public to the Internet. Proxy requests to backend operations also need to be publicly accessible on the Internet. However, you can generate a client-side SSL certificate in Amazon API Gateway to verify that requests to your backend systems were sent by API Gateway using the public key of the certificate.   So API Gateways act as an 'always-on' Internet facing service that sends the HTTP requests to the Zappa Lambda functions by creating \nLambda events.  These events can reach your zappa application even if deployed in a top-secret lockbox.  Note that in the above VPC setup the Lambda function is completely isolated from direct access to the Internet.  Thus the Lambda functions can't make outbound connections to anything: other servers, databases, S3, etc.  Let's face it, there's no real point to going through the effort of doing this if the Lambda functions are really this isolated.  But  the reason this information is important is because in order to leverage other AWS resources, \nthe VPC lays the foundation to do this securely.",
            "title": "The Essential VPC and Subnet Configuration"
        },
        {
            "location": "/aws_network/#extending-the-vpc",
            "text": "To re-iterate: we don't need a VPC network to make the Zappa Lambda-powered Django site visible on the Internet. \nWe need to extend the VPC so that we can add more AWS services like:   Adding an RDS database that is only accessible from our Lambda functions  Adding an S3 bucket   Allowing Lambda functions to communicate to traditional EC2 instances for long running processes  Allowing the Lambda functions to access the Internet to hit an external API  Ability to interact with SQS and/or SNS   The important thing is that we can enable all these scenarios in a secure manner.  There are too many scenarios to go into detail here, so we will provide some guidelines.  Next we learn about the common patterns of VPC usage and try to identify usage options.  One warning before we go on with VPC subnet sizing and Lambda:  When Lambda functions fire, they must be assigned an IP address temporarily.  If you have a lot of Lambda functions firing, then you must have a lot of IP addresses available in your subnet.  You can learn more  here .",
            "title": "Extending the VPC"
        },
        {
            "location": "/aws_network/#vpc-patterns",
            "text": "While the actual VPC itself is very straightforward, the combination of subnets within the VPC can take many forms.  Additionally, you must consider how the subnets will communicate among themselves as well as how  they communicate with outside networks. For a more comprehensive list of options and other valuable information about VPCs, see this link:  https://aws.amazon.com/answers/networking/aws-single-vpc-design/  The options are summarized here:",
            "title": "VPC Patterns"
        },
        {
            "location": "/aws_network/#vpc-with-a-single-internet-accessible-subnet",
            "text": "This pattern places your lambda functions, your RDS, and additional SNS/SQS services in a single subnet that is Internet accessible in your VPC.  In theory you could configure your security groups to ensure only lambda functions can hit your RDS.  One advantage of this setup is that you can setup your local machine to connect to your RDS without a  bastion host .  Just restrict access based on IP.    Important note - you will want to ensure careful inbound IP restrictions.  While it's great that you can connect to RDS with your SQL desktop client, you should setup a  bastion host .  Also note that your zappa deployment  will not have outbound access to the Internet .\nIn order to do this you will have use a Public/Private setup.   Some additional considerations from the  AWS Documentation    We recommend that you avoid DNS resolution of public host names for your VPC. This can take several seconds to resolve, which adds several seconds of billable time on your request. For example, if your Lambda function accesses an Amazon RDS instance in your VPC, launch the instance with the no-publicly-accessible option.    When you add VPC configuration to a Lambda function, it can only access resources in that VPC. If a Lambda function needs to access both VPC resources and the public Internet, the VPC needs to have a Network Address Translation (NAT) instance inside the VPC.    When a Lambda function is configured to run within a VPC, it incurs an additional ENI start-up penalty. This means address resolution may be delayed when trying to connect to network resources.     This scenario is good for straightforward setups with a little work, but has significant limitations",
            "title": "VPC with a single Internet-Accessible subnet"
        },
        {
            "location": "/aws_network/#vpc-with-a-public-subnet-and-private-subnet",
            "text": "Arguably the most flexible and future-proof of all web application setups.  You have two subnets: one public and one private.  Your RDS and lambda functions reside in the private subnet, far away from bad guys, but also far away from your local machine.  In order to access the database from your system you will need a  bastion host  in the public subnet.    Another advantage of this setup is that if you ever want to add additional EC2-based services that need to interact with the Internet you can do this very easily without compromising security.    Generally this setup will require networking knowledge to setup the Internet gateway, bastion host, and NAT Gateway.  The upshot is this configuration is the most secure and most flexible for your growth but will be complex from a network standpoint",
            "title": "VPC with a Public subnet and Private subnet"
        },
        {
            "location": "/aws_network/#on-premises-and-internet-accessible-vpc",
            "text": "Same as the last configuration, but if you have an internal corporate network to connect, you can easily establish a connection to the private subnet without compromising security.  If you thought the last setup was complex, you better know what you are doing from a network standpoint.  A good solution if you need to connect an internal corporate network",
            "title": "On-Premises and Internet-Accessible VPC"
        },
        {
            "location": "/aws_network/#vpc-with-an-internal-only-subnet",
            "text": "Obviously a very special case of creating a Django app for internal use only with no desire to have it accessible by the Internet.  This is actually most secure.  Since you can access any of the resources from your desktop on the internal network.  Not bad for the paranoid or security conscious devops team.  Useful for the simple environment if you have an existing secure network",
            "title": "VPC with an Internal-Only subnet"
        },
        {
            "location": "/aws_network/#subdividing-the-vpc",
            "text": "Once you get a VPC selected you must create subnets within the VPC.  When defining a subnet, you just have to pick a non-overlapping segment of the ip range.  So if you have VPC that spans IP address 10.5.0.1 to 10.5.0.254, then you pick contiguous segments within this range.  See more details in  Primer on AWS VPC Networking",
            "title": "Subdividing the VPC"
        },
        {
            "location": "/aws_network/#examples-for-walkthroughs",
            "text": "For the purposes of walkthroughs, we will leverage a simple VPC with a single subnet.  A single subnet will generally be enough to guide readers through the scenarios.  We have a VPC:   id: vpc-9a9a1dfc  cidr: 10.6.0.0/16   With subnet:   id: subnet-f3446aba  cidr: 10.6.1.0/24   And security group:   id: sg-13a5736f  inbound rules: none  outbound rules: all traffic    TODO: Show example zappa configuration here",
            "title": "Examples for Walkthroughs"
        },
        {
            "location": "/aws_network/#note-on-redundancy",
            "text": "While these examples are all using a single subnet for clarity, in production you will want to create multiple subnets within the VPC all with different availability zones.  This ensures if there is a failure within a single availability zone, there are alternate paths.    The general approach is to associate the Lambda functions with multiple subnets and the AWS resources with the same multiple subnets (e.g. RDS).",
            "title": "Note on Redundancy"
        },
        {
            "location": "/aws_database/",
            "text": "Creating an RDS Database\n\u00b6\n\n\nThere are a number of RDS engines available - https://aws.amazon.com/rds/getting-started/. Another choice is to opt for DynamoDB - https://aws.amazon.com/dynamodb/getting-started/\n\n\nAdding second private subnet to VPC\n\u00b6\n\n\nRDS requires additional subnet to create an instance. Follow step 3 \nCreate Additional Subnets\n\n at https://docs.aws.amazon.com/AmazonECS/latest/developerguide/create-public-private-vpc.html to add one to your existing VPC. Choose an \nAvailability Zone\n different from the first one and \nIPv4 CIDR block\n 10.0.2.0/24.\n\n\nCreating an RDS instance\n\u00b6\n\n\nFollow this guide https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_GettingStarted.CreatingConnecting.PostgreSQL.html keeping in mind parameters we created before.",
            "title": "Creating an RDS Database"
        },
        {
            "location": "/aws_database/#creating-an-rds-database",
            "text": "There are a number of RDS engines available - https://aws.amazon.com/rds/getting-started/. Another choice is to opt for DynamoDB - https://aws.amazon.com/dynamodb/getting-started/",
            "title": "Creating an RDS Database"
        },
        {
            "location": "/aws_database/#adding-second-private-subnet-to-vpc",
            "text": "RDS requires additional subnet to create an instance. Follow step 3  Create Additional Subnets \n at https://docs.aws.amazon.com/AmazonECS/latest/developerguide/create-public-private-vpc.html to add one to your existing VPC. Choose an  Availability Zone  different from the first one and  IPv4 CIDR block  10.0.2.0/24.",
            "title": "Adding second private subnet to VPC"
        },
        {
            "location": "/aws_database/#creating-an-rds-instance",
            "text": "Follow this guide https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_GettingStarted.CreatingConnecting.PostgreSQL.html keeping in mind parameters we created before.",
            "title": "Creating an RDS instance"
        },
        {
            "location": "/aws_route53/",
            "text": "Working with Route53\n\u00b6\n\n\nThis page lists various activites that may be necessary to perform when leveraging Zappa\n\n\nCreate a Hosted Zone in Route53\n\u00b6\n\n\n\n\nNavigate to the \nRoute53 console\n and click \nCreate a Hosted Zone\n\n\nIn the Domain Name field, put only the \nbare\n or \napex\n form of the domain name.  This should be done regardless of the subdomain you wish to host.  For our example we would enter:\n \nDomain Name: zappaguide.com\nType: Public Hosted zone\n\n\n\n\n\nThe console should show you a number of DNS records that you must provide to your Registrar\n \n\n Once you enter these values into your Registar it may take some time for the values to propagate depending on the prior settings of the Registrar.  In this case your domain name Registrar is someone like GoDaddy or NameCheap.\n\n\n\n\n\n\nVerify your DNS settings have propagated\n Using some \nonline tools\n confirm that for this domain, the DNS Nameservers are correct.  If they are not updated, it may take some more time for the information to be propagated.\n\n\nAnother useful DNS query tool is \nhttps://dns.google.com",
            "title": "Working with Route53"
        },
        {
            "location": "/aws_route53/#working-with-route53",
            "text": "This page lists various activites that may be necessary to perform when leveraging Zappa",
            "title": "Working with Route53"
        },
        {
            "location": "/aws_route53/#create-a-hosted-zone-in-route53",
            "text": "Navigate to the  Route53 console  and click  Create a Hosted Zone  In the Domain Name field, put only the  bare  or  apex  form of the domain name.  This should be done regardless of the subdomain you wish to host.  For our example we would enter:\n  Domain Name: zappaguide.com\nType: Public Hosted zone   The console should show you a number of DNS records that you must provide to your Registrar\n  \n Once you enter these values into your Registar it may take some time for the values to propagate depending on the prior settings of the Registrar.  In this case your domain name Registrar is someone like GoDaddy or NameCheap.    Verify your DNS settings have propagated\n Using some  online tools  confirm that for this domain, the DNS Nameservers are correct.  If they are not updated, it may take some more time for the information to be propagated.  Another useful DNS query tool is  https://dns.google.com",
            "title": "Create a Hosted Zone in Route53"
        },
        {
            "location": "/aws_acm/",
            "text": "AWS Certificate Manager (ACM)\n\u00b6\n\n\nThis page lists various activites that may be necessary to perform when leveraging Zappa\n\n\nRequest a Certificate\n\u00b6\n\n\nACM provides digital certificates for free but the certificates can only be used with \nElastic Load Balancing and Amazon CloudFront\n.\n\n\n\n\nWarning\n\n\nTo use ACM with Zappa, you must create or import the certificate in the US East (N. Virginia) (us-east-1).  See \nAWS documentation\n for more details.\n\n\n\n\n\n\nNavigate to the \nACM Console\n and click \nRequest a Certificate\n\n\n\n\nIn 'Add a Domain name' enter \n \n\n\nNote that we entered both the 'www' subdomain and the \napex\n of the domain.  This allows users to leverage either url and have it covered with a single certificate.  More info can be found in the AWS ACM documentation on \nRequesting a Certificate\n\n\n\n\nWarning\n\n\nCarefully consider which domains shall be covered by this certificate because once it is validated, you cannot modify the list of domains.  Any changes will require a new certificate to be issued.\n\n\n\n\n\n\n\n\nSelect validation method\n    ACM needs a way to confirm that you own the domain.  So you must select either \nDNS Validation\n \n    or \nEmail validation\n. \n\n\n\n\n\n\nClick on Review and Request\n\n\nYou should see a confirmation message similar to the image below \n(showing email validation):\n\n\n\n\n\n\n\nEmail validation\n\n\nIf you chose email method of domain ownership validation, an email is sent to the registered contact address in the WHOIS for the domain.\n\nIn addition, a few select email addresses are also included.  Full \nvalidation rules\n are posted in ACM Documentation.\n\n\n\n\n\n\nCheck your email for validation links\n\n\nYou should receive at least one email for each domain you entered.  You may actually get multiple email addresses because sometimes registered emails are duplicated for Techincal or Administrative contacts in the WHOIS information.  The emails should be similar to:\n\n\n\n\n\n\n\nClick on all the validation links\n\n\nYou must click on the validation link for every domain name included in step 2 above\n.  The digital certificate will not be issued until all domains have been verified.\n\n\nAnd then:\n\n\n\n\n\n\n\n\n\n\n\nRecord the ARN for the digital certificate\n\n\nYou will use the ARN for other purposes.  The ARN is displayed on the verification page but also in the details page for the certificate in the ACM console.",
            "title": "AWS Certificate Manager (ACM)"
        },
        {
            "location": "/aws_acm/#aws-certificate-manager-acm",
            "text": "This page lists various activites that may be necessary to perform when leveraging Zappa",
            "title": "AWS Certificate Manager (ACM)"
        },
        {
            "location": "/aws_acm/#request-a-certificate",
            "text": "ACM provides digital certificates for free but the certificates can only be used with  Elastic Load Balancing and Amazon CloudFront .   Warning  To use ACM with Zappa, you must create or import the certificate in the US East (N. Virginia) (us-east-1).  See  AWS documentation  for more details.    Navigate to the  ACM Console  and click  Request a Certificate   In 'Add a Domain name' enter \n   Note that we entered both the 'www' subdomain and the  apex  of the domain.  This allows users to leverage either url and have it covered with a single certificate.  More info can be found in the AWS ACM documentation on  Requesting a Certificate   Warning  Carefully consider which domains shall be covered by this certificate because once it is validated, you cannot modify the list of domains.  Any changes will require a new certificate to be issued.     Select validation method\n    ACM needs a way to confirm that you own the domain.  So you must select either  DNS Validation  \n    or  Email validation .     Click on Review and Request  You should see a confirmation message similar to the image below \n(showing email validation):    Email validation  If you chose email method of domain ownership validation, an email is sent to the registered contact address in the WHOIS for the domain. \nIn addition, a few select email addresses are also included.  Full  validation rules  are posted in ACM Documentation.    Check your email for validation links  You should receive at least one email for each domain you entered.  You may actually get multiple email addresses because sometimes registered emails are duplicated for Techincal or Administrative contacts in the WHOIS information.  The emails should be similar to:    Click on all the validation links  You must click on the validation link for every domain name included in step 2 above .  The digital certificate will not be issued until all domains have been verified. \nAnd then:      Record the ARN for the digital certificate  You will use the ARN for other purposes.  The ARN is displayed on the verification page but also in the details page for the certificate in the ACM console.",
            "title": "Request a Certificate"
        },
        {
            "location": "/additional/",
            "text": "Additional Resources\n\u00b6\n\n\nThis site powered by \nmkdocs\n and powered by github. \n\n\nSource documents are here: \nhttps://github.com/edgarroman/zappa-django-guide",
            "title": "Additional"
        },
        {
            "location": "/additional/#additional-resources",
            "text": "This site powered by  mkdocs  and powered by github.   Source documents are here:  https://github.com/edgarroman/zappa-django-guide",
            "title": "Additional Resources"
        }
    ]
}